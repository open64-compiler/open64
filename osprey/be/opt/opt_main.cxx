/*
 *  Copyright (C) 2021 Xcalibyte (Shenzhen) Limited.
 */

/*
 * Copyright (C) 2008-2011 Advanced Micro Devices, Inc.  All Rights Reserved.
 */

//-*-c++-*-

/*
 * Copyright 2002, 2003, 2004, 2005, 2006 PathScale, Inc.  All Rights Reserved.
 */

// ====================================================================
// ====================================================================
//
// Module: opt_main.cxx
// $Revision: 1.1.1.1 $
// $Date: 2005/10/21 19:00:00 $
// $Author: marcel $
// $Source: /proj/osprey/CVS/open64/osprey1.0/be/opt/opt_main.cxx,v $
//
// ====================================================================
//
// Copyright (C) 2000, 2001 Silicon Graphics, Inc.  All Rights Reserved.
//
// This program is free software; you can redistribute it and/or modify
// it under the terms of version 2 of the GNU General Public License as
// published by the Free Software Foundation.
//
// This program is distributed in the hope that it would be useful, but
// WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
//
// Further, this software is distributed without any warranty that it
// is free of the rightful claim of any third person regarding
// infringement  or the like.  Any license provided herein, whether
// implied or otherwise, applies only to this software file.  Patent
// licenses, if any, provided herein do not apply to combinations of
// this program with other software, or any other product whatsoever.
//
// You should have received a copy of the GNU General Public License
// along with this program; if not, write the Free Software Foundation,
// Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
//
// Contact information:  Silicon Graphics, Inc., 1600 Amphitheatre Pky,
// Mountain View, CA 94043, or:
//
// http://www.sgi.com
//
// For further information regarding this notice, see:
//
// http://oss.sgi.com/projects/GenInfo/NoticeExplan
//
// ====================================================================
// ====================================================================


/*

  HISTORY OF THE GLOBAL OPTIMIZER
  -------------------------------
  
  This is a brief history of the development of the Mongoose global
  optimizer project at SGI, with acknowledgment of the key contributors
  of the project. In 1994, as the compiler group at SGI was finishing
  the Ragnarok compiler for the MIPS R8000 processor, which was designed
  to address the scientific applications, the group recognized several
  limitations of the Ragnarok compiler.  In the second half of 1994, the
  group set out to design a new compiler that not only produces highly
  efficient code for both scientific and non-scientific applications,
  but also must be stable and fast enough to be used for day-to-day
  development of large applications.  The group also decided to overhaul
  the infrastructure of the compiler to take advantage of the latest
  advances in compiler technology.
  
  The global optimizer is not only designed to be a major optimization
  phase in the compiler, but also as a utility component that can be 
  invoked by other phases.  The IPO (inter-procedural optimization) 
  and LNO (loop-nest optimization) phases rely heavily on the global 
  optimizer for pre-conditioning the code, and their analyses and 
  processing depend on use-def and alias information generated by and 
  fed to them by the optimizer.  Design discussions on the new compiler 
  started around August. 1994. Actual development started in Oct., 1994.  
  The following is a (probably incomplete) list of features with dates 
  of the Mongoose optimizer releases.
  
  
  Release 7.0, May 1996.
  
  Most functionalities are finished in 7.0.  The major decision was to
  use SSA as *the* IR in the optimizer, contrary to using SSA form as an
  augmentation to the IR in many other compilers.  It leads to the
  development of HSSA form, and extensions to represent indirect aliases
  in SSA form.  The optimizer group also made the conscious decision to 
  divide the optimization algorithms used into two different types: 
  SSA-based and the bitvector-based.  At the time, it was not clear how 
  to perform an important optimization - partial redundancy elimination -
  in terms of SSA form, so we used the old and tried bitvector approach.
  When the compiler is released, its performance has already surpassed 
  those of the Ucode compiler and Ragnarok compiler. 
  
  Features of the release include:
  
  Symbol table & alias analysis
  * optimizer-centric symbol table
  * points-to based alias analysis
  * rule-based alias analysis for extensibility
  
  SSA 
  * HSSA form (hash-based value numbering)
  * efficient representation of aliases and indirect memory operation in SSA
  * use of zero SSA version
  * refinement of points-to analysis using SSA 
  * copy propagation without overlapping variable life times
  * induction variable recognition
  * do-loop recognition
  * dead code elimination
  * support of C++ exception handling
  
  Bitvector
  * partial redundancy elimination
  * strength reduction
  * linear function test replacement
  * induction variable elimination
  * register variable identification (register promotion)
  
  
  Release 7.1, Nov 1996.
  
  The development between 7.1 and 7.0 was relatively short.  The release
  focused on improving the stability of the optimizer, fine-tuning the
  optimizations, and improving the debuggability of the optimizer.  For
  example, we introduced heavy uses of assertions and verification
  routines in the optimizer, maximized code re-uses, and collected many
  test cases for optimizer bugs into a regression test suite.
  
  
  Release 7.2, Aug 1997.
  
  The major issue tackled in 7.2 was the undesirable separation between
  SSA-based optimizations and bitvector-based optimizations.  As
  discussed earlier, PRE was the only important optimization that did
  not have a known equivalent algorithm in the SSA domain.  The
  optimizer group was determined to solve the problem.  The outcome is
  the invention of the SSAPRE algorithm.  Strength reduction and linear
  function test replacement were later integrated into the SSAPRE
  algorithm.  In addition, we found that register promotion could
  actually be formulated as a pair of redundancy elimination problems.
  The 7.2 optimizer can compile large applications faster than 7.1 due
  to smaller memory footprint made possible by the use of SSA-based
  optimizations throughout the optimizer.  In the software engineering
  side, we started to move towards using generic programming techniques
  (using the template feature in the C++ language) to maximize
  code-reuse.  The SSAPRE implementation displays a high degree of
  code-reuse because its coding infrastructure is shared by 3 different
  optimizations based on the SSAPRE framework.  By virtue of the 3
  different incarnations of the SSAPRE concept as 3 different
  optimizations, SSAPRE is also an good example of concept-reuse.
  
  New features of 7.2 include:
  
  * alias classification: based on Steensgard's almost linear-time
    points-to analysis
  * invention of the sparse approach to PRE
      * strength reduction
      * linear function test replacement
      * busy code motion (code hoisting)
      * load and store optimization for register promotion of both scalar and 
        indirect variables (the second register promotion phase RVI2 is still
        implemented via bit vectors due to lack of manpower resources to do
        the conversion)
      * aggressive code motion (speculation of expressions)
  * second-order effect of PRE (new optimization opportunities exposed by PRE)
  * F90 support
  * new scheme for CVT optimizations 
    (important for 32 bit variables on 64-bit processors)
  * restrict pointer support
  * generalization of REGION: use optimization regions to partition program
    into smaller optimization units; exception region to support C++
    exception handling with optimizations enabled; MP region to support
    parallelization.
  * some work to avoid exponential optimization time due to the formation of
    huge trees during optimization
  * IPA support: generate sliced SSA to build IPA summary information
  
  
  Release 7.3,  May 1999
  
  At the end of release 7.2, all major parts of the optimizer input and
  output HSSA form.  This laid the foundation for performing iterative
  optimizations for Release 7.3.  As a result of iterating optimizations, 
  a 1.0 ratio for the abstraction penalty benchmark was achieved.  
  Furthermore, we started adapting the SSAPRE algorithm to deal with value 
  redundancy as well as lexical redundancy.  However, only full value 
  redundancy are optimized at the end of 7.3.  We expect extension to 
  cover partial value redundancy to be finished in future releases.  
  
  New features of 7.3 includes:
  
  * iterative global value numbering (GVN)
  * full value redundancy elimination (based on GVN)
  * live-range shrinking
     (an extension to load/store optimization in the SSAPRE framework)
  * bitwise dead and redundant code elimination 
     (elimination of dead and redundant bitwise operationss)
  * control flow optimization
     (remove partial and full redundant branches by code duplication)
  * iterative optimizations
     (capture the second-order effect optimizations)
  * linear function test replacement enhancements
  * new profile data update scheme
  
  
  SGIpro 1.0, not released yet.
  
  After release 7.3, the group started to focus on the next compiler for
  the IA-64 architecture.  Most resources were concentrated on the
  architecture-dependent portions of the compilers.  Relative few
  enhancements were made to the optimizer.  New features in osprey 1.0:
  
  * IA-64 convert opcode optimizations
     * an extension of the bitwise optimization framework done in 7.3
  * perform lowering within the optimizer to expose CVTLs specific to IA-64
  * introduce boolean data type (MTYPE_B) for better use of predicate registers 
  * new scheme to optimize bit-field accesses
     * a bit-field variable is considered an individual scalar in
       earlier phases to benefit from all scalar optimizations
     * it is lowered into the load/store of the enclosing words followed by
       bit extraction or bit composition to allow for redundancy elimination
       and load/store optimizations.
  
  
  People
  
  When the global optimizer project was started, there were four team members.
  Sun Chan has been the manager of the global optimizer group throughout most
  of its development.  His building of a high quality optimizer team and 
  his setting of high expectations are the keys to the success of the 
  project.  Fred Chow is not only the most important contributor of the 
  optimizer, but also the architect of the Mongoose compiler.  His many 
  years of experience in developing the MIPS Ucode compiler has kept us 
  on the right track.  Raymond Lo has contributed significantly to the 
  architecture of the optimizer, and has worked on almost every part of 
  the optimizer.  He succeeds Sun Chan as the manager of the optimizer group.
  Shin-Ming Liu was among the original team members, and he laid the
  overall programming methodology and coding style.  Over time, we have
  recruited additional team members that have contributed to different
  parts of the project.  The global optimizer would nowhere be as good as 
  it is today without the dedicated work of these top compiler engineers.  
  Here is a list of all the people who have contributed significantly to the 
  global optimizer, in alphabetical order:
  
  	Sun Chan
  	Fred Chow 
  	Peter Dahl
  	Rune Dahl 
  	Robert Kennedy
  	Shin-Ming Liu
  	Raymond Lo
  	David Stephenson
  	Mark Striech
  	Peng Tu
  
  
  Publications
  
  Robert Kennedy, Fred Chow, Sun Chan, Shin-Ming Liu, Raymond Lo, and
  Peng Tu, "Partial Redundancy Elimination in SSA form", ACM TOPLAS, May
  1999.
  
  Raymond Lo, Fred Chow, Robert Kennedy, Shin-Ming Liu, and Peng Tu,
  "Register Promotion by Sparse Partial Redundancy Elimination of Loads
  and Stores", Proceedings of the ACM SIGPLAN Conference on Programming
  Language Design and Implementation (PLDI-98), June 17-19, 1998.
  
  Robert Kennedy, Fred Chow, Peter Dahl, Raymond Lo, and Mark Streich,
  "Strength Reduction via SSAPRE", International Conference on Compiler
  Construction (CC '98), Lisbon, Portugal, March 30, 1998.
  
  Fred Chow, Sun Chan, Robert Kennedy, Shin-Ming Liu, Raymond Lo and
  Peng Tu, "A New Algorithm for Partial Redundancy Elimination based on
  SSA Form", Proceedings of the ACM SIGPLAN `97 Conference on
  Programming Language Design and Implementation, pp. 273-286, June
  15-18, 1997.
  
  S.-M. Liu, R. Lo and F. Chow, "Loop Induction Variable
  Canonicalization in Parallelizing Compiler", Intl. Conf. on Parallel
  Architectures and Compilation Techniques (PACT 96), Oct 1996.
  
  Fred Chow, Sun Chan, Shin-Ming Liu, Raymond Lo, and Mark Streich,
  "Effective Representation of Aliases and Indirect Memory Operations in
  SSA Form", 6th Intl. Conf.  on Compiler Construction (CC'96),
  pp. 253-267,Apr 1996.
  
*/ 


#ifdef USE_PCH
#include "opt_pch.h"
#endif // USE_PCH
#pragma hdrstop


#define opt_main_CXX	"opt_main.cxx"

#include <stdint.h>
#define USE_STANDARD_TYPES
#include <alloca.h>
#include "pu_info.h"		/* for PU_Info_state and related things */

#include "unistd.h"

#include "defs.h"
#include "erbe.h"		/* Error messages		*/
#include "glob.h"		/* for Feedback_File_Name	*/
#include "config_targ.h"
#include "config_ipa.h"         /* IPA_Enable_Alias_Class       */
#include "config.h"		/* Query_Skiplist		*/
#include "wn.h"
#include "wn_simp.h"
#include "wn_lower.h"
// Remove the following line
#include "wn_util.h"

#include "ir_reader.h"
#include "tracing.h"
#include "be_util.h"
#include "region_util.h"
#include "fb_whirl.h"		// for FB_Annotate_whirl

#include "optimizer.h"
#include "opt_alias_class.h"
#include "opt_cfg.h"
#include "opt_main.h"
#include "opt_fb.h"
#include "opt_exc.h"
#include "opt_sym.h"
#include "opt_ssa.h"
#include "opt_emit.h"
#include "opt_du.h"

#include "opt_dbg.h"
#include "opt_goto.h"
#include "opt_rvi.h"
#include "opt_util.h"
#include "opt_alias_mgr.h"
#include "opt_alias_interface.h"	/* for Verify_alias() */
#include "opt_vn.h"                     /* Global value numbering (gvn) */
#include "opt_fold.h"

#include "config_lno.h"
#include "config_opt.h"			// for Delay_U64_Lowering
#include "config_vsa.h"

#include "dep_graph.h"			/* for tracing Current_Dep_Graph */
#include "wb_ipl.h"			/* whirl browser for ipl */ 
#include "opt_du.h"

#ifndef __MINGW32__
#include "regex.h"                      // For regcomp and regexec
#endif /* __MINGW32__ */
#include "xstats.h"                     // For PU_WN_BB_Cnt
#include "opt_wovp.h"     // for write once variable promotion
#include "opt_misc.h"
#include "opt_lmv.h"
#include "opt_peel_unroll.h"

#if defined(TARG_SL)
#include "opt_lclsc.h"
#endif

#if defined(BUILD_MASTIFF)
#include "opt_dna.h"
#include "opt_cda.h"
#include "opt_vra.h"
#include "opt_vsa_eh.h"
#endif

#include "wssa_utils.h"   // WHIRL SSA

#include "../opt/init.cxx"          // force include of Wopt_Initializer

extern "C" void
Perform_Procedure_Summary_Phase (WN* w, struct DU_MANAGER *du_mgr,
				 struct ALIAS_MANAGER *alias_mgr,
				 EMITTER *emitter);
#if defined(__linux__) || defined(BUILD_OS_DARWIN) || !defined(SHARED_BUILD)
extern void (*Perform_Procedure_Summary_Phase_p) (WN*, DU_MANAGER*,
						  ALIAS_MANAGER*, void*);
#define Perform_Procedure_Summary_Phase (*Perform_Procedure_Summary_Phase_p)
#else
#pragma weak Perform_Procedure_Summary_Phase
#endif // __linux__

extern BOOL Enable_WN_Simp;
extern void Simplify_bool_expr(COMP_UNIT *);
extern void WN_unroll(WN *);

static MEM_POOL  Opt_global_pool;
static MEM_POOL  Opt_preopt_pool;
static MEM_POOL  Opt_local_pool;
static IPSA     *IPSA_manager = NULL;
static ST       *Opt_current_pu_st = NULL;
static PU_IDX    Opt_current_pu;

static void identify_complete_struct_relayout_candidates(WN *wn);
static void identify_array_remapping_candidates(WN *wn, ST *loop_index_st);

extern void 
remove_redundant_mem_clears(WN *func,
                            ALIAS_MANAGER *alias_mgr,
                            DU_MANAGER *du_mgr);

static BOOL Opt_memory_initialized = FALSE;
static void Init_opt_memory_pools(void)
{
  if (Opt_memory_initialized) return;

  OPT_POOL_Initialize(&Opt_global_pool, "Opt_global_pool", TRUE,
		      MEM_DUMP_FLAG + 1);
  OPT_POOL_Initialize(&Opt_preopt_pool, "Opt_preopt_pool", TRUE,
		      MEM_DUMP_FLAG + 1);
  OPT_POOL_Initialize(&Opt_local_pool,  "Opt_local_pool",  TRUE,
		      MEM_DUMP_FLAG + 1);
  OPT_POOL_Push(&Opt_global_pool, MEM_DUMP_FLAG+1);
  OPT_POOL_Push(&Opt_local_pool, MEM_DUMP_FLAG+1);

  Opt_memory_initialized = TRUE;
}

static void Terminate_opt_memory_pools(void)
{
  if (!Opt_memory_initialized) return;

  OPT_POOL_Pop(&Opt_global_pool, MEM_DUMP_FLAG+1);
  OPT_POOL_Pop(&Opt_local_pool, MEM_DUMP_FLAG+1);
  OPT_POOL_Delete(&Opt_global_pool, MEM_DUMP_FLAG+1);
  OPT_POOL_Delete(&Opt_local_pool, MEM_DUMP_FLAG+1);

  Opt_memory_initialized = FALSE;
}

static void Manage_pu_level_memory(COMP_UNIT *comp_unit, MEM_POOL *pool)
{
  if (comp_unit->Phase() == PREOPT_PHASE) {
    CXX_DELETE(comp_unit, &Opt_preopt_pool);
    OPT_POOL_Pop(&Opt_preopt_pool, MEM_DUMP_FLAG+1);
  }

#ifdef BUILD_MASTIFF
  if (IPSA_manager != NULL) return;
#endif

  CXX_DELETE(comp_unit, &Opt_global_pool);
  Terminate_opt_memory_pools();
}

#if defined(BUILD_MASTIFF)
void Init_ipsa_context(void)
{
  Init_opt_memory_pools();
  IPSA_manager = CXX_NEW(IPSA(), Malloc_Mem_Pool);
  g_ipsa_manager = IPSA_manager;
}

void IPSA_analyze(void)
{
  OPT_POOL_Delete(&Opt_preopt_pool, MEM_DUMP_FLAG+1);
  IPSA_manager->Transition_state();
  IPSA_manager->Analyze();
}

void IPSA_emit(char *workdir)
{
  if(!IPSA_manager) return;
  IPSA_manager->Emit(workdir);
}

BOOL IPSA_insession(void)
{
  return (Run_vsaopt && IPSA_manager != NULL);
}

DNA_NODE* Get_cur_dna(void)
{
  return IPSA_manager ? IPSA_manager->Cur_dna() : NULL;
}

void IPSA_verify(void)
{
  IPSA_manager->Verify();
}

void IPSA_build_cha_begin(void)
{
  if(!IPSA_manager) return;
  IPSA_manager->Build_cha_begin();
}

void IPSA_build_cha(void)
{
  if(!IPSA_manager) return;
  IPSA_manager->Build_and_merge_cha();
}

void IPSA_build_cha_end(void)
{
  if(!IPSA_manager) return;
  IPSA_manager->Build_cha_end();
}

void Terminate_ipsa_context(void)
{
  CXX_DELETE(IPSA_manager, Malloc_Mem_Pool);
  Terminate_opt_memory_pools();
  IPSA_manager = NULL;
}
#endif

static void Opt_set_current_pu_name(WN *wn_tree)
{
  Is_True(wn_tree != NULL && (WN_opcode(wn_tree) == OPC_REGION ||
			      WN_opcode(wn_tree) == OPC_FUNC_ENTRY),
	  ("Opt_set_current_pu_name, unknown WN"));
  ST *st;
  if (WN_opcode(wn_tree) == OPC_REGION) {
    // find the func_entry
    WN *pu = REGION_find_pu(wn_tree);
    Is_True(pu != NULL && WN_opcode(pu) == OPC_FUNC_ENTRY,
	    ("Opt_set_current_pu_name, could not find func entry"));
    st = WN_st(pu);
  } else {
    st = WN_st(wn_tree);
  }

  if (Opt_current_pu_st == NULL) {
    Opt_current_pu_st = st;
    Opt_current_pu = ST_pu(st);
  }
  else if (st != Opt_current_pu_st) {
    Opt_current_pu_st = st;
    Opt_current_pu = ST_pu(st);
  }
  else {
  }
}

// ====================================================================
// class WOPT_SWITCHES
//   This class is designed to save the original global WOPT_Enable_*
//   flags.  Once all flags are saved, we can then freely adjust all
//   flags within the optimizer.  But, before we exit the optimizer,
//   we restore the global flags to their original state.
//   For the nature of this work, we use the constructor and
//   destructor of this class to call two private functions:
//
//     Adjust_Optimization(void)
//       and
//     Unadjust_Optimization(void)
//
//   The major advantage to use the destructor, is the C++ compiler
//   help assuring that the original state are restored.
//
//   Whenever you add a new flag, please follow the existing setup:
//     a. add a member, for this new flag, follow the naming
//        convention.
//     b. add a line in the constructor, to save the value in this new
//        flag. For the destructor, restore the original value.
//     c. add to Adjust_Optimization and Unadjust_Optimization, if you
//        need to adjust the flag for different phase.
//     d. all flags are in alphabetical order.  It makes easier to
//        update.
// ====================================================================
class WOPT_SWITCHES {
private:
  OPT_PHASE _phase;
  INT32     _pragma_flags;
  BOOL  _add_do_loop_info;
  BOOL  _add_label_loop_info;
  BOOL  _addr;
  BOOL  _aggressive_dce;
  BOOL  _aggressive_code_motion;
  BOOL  _alias_classification;
  BOOL  _alias_class_fortran_rule;
  BOOL  _alias_pointer_parms;
  BOOL  _bits_load_store;
  BOOL  _combine_operations;
  BOOL  _call_zero_version; 
  BOOL  _compare_simp;
  INT32 _copy_propagate;
  BOOL  _copy_prop_into_array;/* propagate array ref into array index */
  BOOL  _crsimp;         /* coderep simplifier				*/
  BOOL  _dce;
  BOOL  _dce_alias;
  BOOL  _dce_label;
  BOOL  _dse_aggressive;
  BOOL  _du_full;	/* full DU info for indirect references */
  BOOL  _edge_placement;
  BOOL  _exp_pre;
  BOOL  _fold2const;	/* simplification during copy propagation	*/
  BOOL  _lno_copy;
  BOOL  _fsa;
  BOOL  _goto;
  BOOL  _iload_prop;
  BOOL  _input_prop;
  BOOL  _itself_prop;
  BOOL  _ivar_common;
  BOOL  _ive;		/* induction-var elimination */
  BOOL  _ivr;		/* induction-var recognition */
  BOOL  _ldx;            /* index load optimization */
  BOOL  _lego_opt;
  BOOL  _load_pre;
  BOOL  _local_rvi;
  BOOL  _ocopy;
  BOOL  _parm;		/* insert optparm nodes over call parms */
  BOOL  _phi_simp;
  BOOL  _prop_aggressive; 
  BOOL  _replace_second_iv;
  BOOL  _restricted_map;
  BOOL  _rvi;		/* reg-var identification			*/
  BOOL  _simp_iload;
  BOOL  _slt;
  BOOL  _store_pre;
  BOOL  _ssa_pre;
  BOOL  _tail_recur;
  INT32 _trip;
  BOOL  _update_vsym;
  INT32 _value_numbering;  /* 0==off, 1==single-pass, 2==iterative */
  BOOL  _verbose;
  INT32 _verify;	/* verify data structures      	                */
  BOOL  _vn_full;
  BOOL  _vsa_aob;       /* Array out of bound */
  BOOL  _vsa_npd;       /* Null pointer dereference */
  BOOL  _vsa_rbc;       /* Rule based check */
  BOOL  _vsa_uaf;       /* Use after free */
  BOOL  _vsa_uiv;       /* Uninitialized variable reference */
  BOOL  _vsa_ubf;
  BOOL  _vsa_msf;
  BOOL  _vsa_ddv;
  BOOL  _vsa_ral;
  BOOL  _vsa_rvsa;
  BOOL  _vsym_unique;
  BOOL  _warn_uninit;
  BOOL  _while_loop;	/* cvt while-do to do-loop			*/
  BOOL  _wn_simp;	/* WHIRL node simplifier			*/
  BOOL  _whirl_ssa;     /* emit WHIRL SSA in WOPT emitter               */
  BOOL  _wovp; /* Write-once variable promotion   */
  BOOL  _zero_version;
  BOOL  _epre_before_ivr; // For running epre early
  BOOL  _lpre_before_ivr; // For running lpre early
  BOOL  _spre_before_ivr; // For running spre early
  BOOL  _bdce_before_ivr; // For running bdce early
  BOOL  _loop_multiver;   // loop multiversioning 
  BOOL  _useless_store_elimination; // eliminate useless store in a loop
  BOOL _pro_loop_fusion_trans; 
  BOOL _pro_loop_interchange_trans;
  BOOL _pro_loop_ext_trans;
  BOOL _mem_clear_remove;
  BOOL _bool_simp;
  BOOL _fold_lda_iload_istore;
  BOOL _no_return;
  BOOL _nothrow;
  BOOL _simp_if_conv;
  BOOL _eh_cfg_opt;

  WOPT_SWITCHES(const WOPT_SWITCHES&);
  WOPT_SWITCHES& operator = (const WOPT_SWITCHES&);
  void Adjust_Optimization(void) {
    switch (_phase) {
    case PREOPT_PHASE:
      if (Run_vsaopt) {
	  WOPT_Enable_Bits_Load_Store = FALSE;
	  WOPT_Enable_Input_Prop = FALSE; // we perform vsa after coderep creation
	  WOPT_Enable_IVR = TRUE;
          //let -VSA:aob|npd|uaf|uiv|ral|dead to control these flags
	  //VSA_Aob = TRUE;
	  //VSA_Npd = TRUE;
	  //VSA_Uaf = FALSE;
	  //VSA_Uiv = TRUE;
	  //VSA_Ral = TRUE;
	  //VSA_Ddv = TRUE;
      }
      else {
          VSA_Alias_Local_Lda = TRUE;  // force LDA on local var aliased with call
      }
      break;
    case PREOPT_DUONLY_PHASE:
      /* set all optimizations off */
      WOPT_Enable_Add_Do_Loop_Info =
      WOPT_Enable_Add_Label_Loop_Info =
      WOPT_Enable_Aggressive_dce =
      WOPT_Enable_Aggressive_Code_Motion =
      WOPT_Enable_Alias_Classification =
      WOPT_Enable_Alias_Class_Fortran_Rule =
      WOPT_Enable_Combine_Operations =
      WOPT_Enable_Compare_Simp =
      WOPT_Enable_CRSIMP =
      WOPT_Enable_DCE =
      WOPT_Enable_DCE_Alias =
      WOPT_Enable_Edge_Placement =
      WOPT_Enable_Exp_PRE =
      WOPT_Enable_Fold2const =
      WOPT_Enable_LNO_Copy_Propagate =
      WOPT_Enable_FSA =
      WOPT_Enable_Goto =
      WOPT_Enable_Input_Prop =
      WOPT_Enable_Itself_Prop =
      WOPT_Enable_Ivar_Common =
      WOPT_Enable_IVE =
      WOPT_Enable_IVR =
      WOPT_Enable_Ldx =
      WOPT_Enable_Load_PRE =
      WOPT_Enable_Local_Rvi =
      WOPT_Enable_Output_Copy = 
      WOPT_Enable_Parm =
      WOPT_Enable_Phi_Simp =
      WOPT_Enable_RVI =
      WOPT_Enable_SLT =
      WOPT_Enable_Store_PRE =
      WOPT_Enable_SSA_PRE =
      WOPT_Enable_Vsym_Unique =
      OPT_Enable_WHIRL_SSA =
      WOPT_Enable_WOVP =
      Enable_WN_Simp =			// disable WHIRL simplifier
      // WOPT_Enable_Zero_Version =
      WOPT_Enable_Tail_Recur =
	FALSE;

      WOPT_Enable_Copy_Propagate = TRUE;
      WOPT_Enable_Verify = 1;
      break;

    case PREOPT_IPA0_PHASE:
    case PREOPT_IPA1_PHASE:
      WOPT_Enable_Call_Zero_Version = FALSE;
      WOPT_Enable_Combine_Operations = FALSE;
      WOPT_Enable_Goto = FALSE;
//      OPT_Enable_WHIRL_SSA = FALSE;
      WOPT_Enable_WOVP = FALSE;
      WOPT_Enable_Tail_Recur = FALSE;
      break;

    case MAINOPT_PHASE:
      WOPT_Enable_While_Loop = FALSE;

      // disable edge placement if new pre is off
      if (! WOPT_Enable_SSA_PRE) {
	WOPT_Enable_Edge_Placement = FALSE;
	WOPT_Enable_Backedge_Placement = FALSE;
      }
      else {
        WOPT_Enable_RVI1 = FALSE;
        if (!WOPT_Enable_Exp_PRE && WOPT_Enable_Load_PRE) {
          // need to establish var phi hash
          WOPT_Enable_Exp_PRE = TRUE;
          WOPT_Enable_Exp_PRE_Limit = 0;
        }
      }

      if (Opt_Level > 2 && PU_mp_needs_lno(Get_Current_PU())) {
	WOPT_Enable_Lego_Opt = TRUE;
      }

      if (Opt_Level <= 2) {
	// off for -O2, turned on at -O3, PV 507356
	WOPT_Enable_Replace_Second_IV = FALSE;

	// improve compile time
	if (!WOPT_Enable_Extra_Rename_Pass_Set)
	  WOPT_Enable_Extra_Rename_Pass = 1;
      }

      if (WOPT_Enable_Lego_Opt) {
	WOPT_Enable_Compare_Simp = TRUE;
	WOPT_Enable_Replace_Second_IV = TRUE;
	WOPT_Enable_LFTR_Ivar = TRUE;
	// WOPT_Enable_Iload_Prop = FALSE;
      }

      // if the global flag "Allow_wrap_around_opt" is off
      if ( ! Allow_wrap_around_opt ) {
        WOPT_Enable_LFTR2 = FALSE;
        WOPT_Enable_Compare_Simp = FALSE;
        WOPT_Enable_IVR = FALSE;
      }

      // disable WHIRL simplifier
      Enable_WN_Simp = FALSE;

      // always disable LNO copy prop
      WOPT_Enable_LNO_Copy_Propagate = FALSE;

      // only combine operations in the main optimizer phase
      WOPT_Enable_Combine_Operations = _combine_operations;

      // do not get rid of useless labels (yet?) during mainopt
      WOPT_Enable_DCE_Label = FALSE;

      // allow prop of array refs into array indexes during mainopt
      if ( ! WOPT_Enable_Copy_Prop_Ops_Into_Array_Set )
	WOPT_Enable_Copy_Prop_Ops_Into_Array = TRUE;

      if (WOPT_Enable_Feedback_LPRE || 
          WOPT_Enable_Feedback_EPRE ||
          OPT_Enable_WHIRL_SSA)
	WOPT_Enable_Zero_Version = FALSE;

      if (!OPT_Enable_EH_CFG_OPT_Set)
        OPT_Enable_EH_CFG_OPT = TRUE;

      // Disable all makor opts when we are running vsa 
      if (Run_vsaopt) {
	  WOPT_Bottom_Test_Loop_Check = FALSE;
	  WOPT_Enable_Aggressive_Code_Motion = FALSE;
	  WOPT_Enable_Aggr_Invariant == FALSE;
	  WOPT_Enable_Aggressive_Lftr == FALSE;
	  WOPT_Enable_Autoaggstr_Reduction_Threshold = 0;
	  WOPT_Enable_Bitwise_DCE = FALSE;
	  WOPT_Enable_Bits_Load_Store = FALSE;
	  WOPT_Enable_CFG_Opt = FALSE; WOPT_Enable_Compare_Simp = FALSE;
	  WOPT_Enable_CSE_FP_comparison = FALSE;
	  WOPT_Enable_Const_PRE = FALSE;
	  WOPT_Enable_DIVREM = FALSE;
	  WOPT_Enable_Edge_Placement = WOPT_Enable_Backedge_Placement = WOPT_Enable_Exp_PRE = FALSE;
	  OPT_Enable_EH_CFG_OPT = FALSE;
	  WOPT_Enable_Hoisting = FALSE;
	  WOPT_Enable_Invariant_Loop_Bounds = FALSE;
	  WOPT_Enable_Ivar_Hoisting = FALSE;
	  WOPT_Enable_Ivar_PRE = FALSE;
	  WOPT_Enable_IVE_Old = FALSE;
	  //WOPT_Enable_IVR = FALSE;    /* enable IVR for value range analysis */
	  WOPT_Enable_LFTR_Ivar = FALSE;
	  WOPT_Enable_LFTR2 = FALSE;
	  WOPT_Enable_Load_PRE = FALSE;
	  WOPT_Enable_Loopinvarexp_Str_Reduction = FALSE;
	  WOPT_Enable_Local_Rvi = FALSE;
	  WOPT_Enable_Move_Intrinsicop = FALSE;
	  WOPT_Enable_New_SR = FALSE;
	  WOPT_Enable_Parm = FALSE;
	  WOPT_Enable_Pro_Loop_Fusion_Trans = FALSE;
	  WOPT_Enable_Pro_Loop_Interchange_Trans = FALSE;
	  WOPT_Enable_Pro_Loop_Limit = -1;
	  WOPT_Enable_Reassociation_CSE = FALSE;
	  WOPT_Enable_Replace_Second_IV = FALSE; 
	  WOPT_Enable_Rsv_Bits = 0;
	  WOPT_Enable_RVI = FALSE;
	  WOPT_Enable_SLT = FALSE;
	  //WOPT_Enable_Simple_If_Conv = 0; /* enable if-conv for simplify JAVA boolean expression */
	  WOPT_Enable_SSA_PRE = FALSE;
	  WOPT_Enable_Store_PRE = FALSE;
	  WOPT_Enable_Shrink = FALSE;
	  WOPT_Enable_Tail_Recur = FALSE;
	  WOPT_Enable_Value_Numbering = FALSE;
	  WOPT_Enable_Verify = 0;
	  WOPT_Enable_VN_Full = FALSE;
	  WOPT_Enable_Vn_Ivc = 0;
	  WOPT_Enable_WN_Unroll = 0;
	  WOPT_Enable_WOVP = FALSE;
	  WOPT_Simplify_Bit_Op = FALSE;
	  WOPT_Enable_Tail_Recur = FALSE;
      } // end Run_vsaopt
      else {
          VSA_Alias_Local_Lda = TRUE;  // force LDA on local var aliased with call
      }
      break; // end MAINOPT_PHASE

#ifdef TARG_NVISA
    case PREOPT_CMC_PHASE:
      WOPT_Enable_Goto = TRUE;
      WOPT_Enable_Call_Zero_Version = FALSE;
      WOPT_Enable_Zero_Version = FALSE;
      WOPT_Enable_DU_Full = TRUE;
      WOPT_Enable_Copy_Propagate = TRUE;
      break;
#endif

    case PREOPT_LNO1_PHASE: 
      WOPT_Enable_SSA_PRE = FALSE;
      WOPT_Enable_Goto = FALSE;
      WOPT_Enable_Epre_Before_Ivr = FALSE;
      WOPT_Enable_Lpre_Before_Ivr = FALSE;
      WOPT_Enable_Spre_Before_Ivr = FALSE;
      WOPT_Enable_Bdce_Before_Ivr = FALSE;
      WOPT_Enable_IVR = FALSE;
      WOPT_Enable_Copy_Propagate = FALSE;
      WOPT_Enable_DCE = FALSE;
      WOPT_Enable_Bool_Simp = FALSE;
      WOPT_Enable_Fold_Lda_Iload_Istore = FALSE;
      WOPT_Enable_Input_Prop = FALSE;
      OPT_Enable_WHIRL_SSA = FALSE;
  
    case PREOPT_LNO_PHASE: 
      if (Run_autopar && Current_LNO->IPA_Enabled
#ifdef KEY // bug 6383
	  && PU_WN_BB_Cnt < 2000
#endif
	  ) { 
	WOPT_Enable_Call_Zero_Version = FALSE;
	WOPT_Enable_Zero_Version = FALSE;
	WOPT_Enable_DU_Full = TRUE;
      } 

      if (WOPT_Enable_Pro_Loop_Fusion_Trans
	  || WOPT_Enable_Pro_Loop_Interchange_Trans
	  || WOPT_Enable_Pro_Loop_Ext_Trans) {
	WOPT_Enable_Noreturn_Attr_Opt = FALSE;
      }
      
      // fall though 
    default:
      // if the global flag "Allow_wrap_around_opt" is off
      if ( ! Allow_wrap_around_opt ) {
        WOPT_Enable_Compare_Simp = FALSE;
        WOPT_Enable_IVR = FALSE;
      }
      WOPT_Enable_Combine_Operations = FALSE;
      WOPT_Enable_SLT = FALSE;
      OPT_Enable_WHIRL_SSA = FALSE;
      break;
    } // switch
    WOPT_Enable_Ldx = Indexed_Loads_Allowed;

    if (_pragma_flags & WOPT_TAIL_RECUR_FINISHED)
      WOPT_Enable_Tail_Recur = FALSE;

    if (_phase != MAINOPT_PHASE || !WOPT_Enable_New_Phase_Ordering)
    {
      WOPT_Enable_Epre_Before_Ivr = FALSE; // For running epre early
      WOPT_Enable_Lpre_Before_Ivr = FALSE; // For running lpre early
      WOPT_Enable_Spre_Before_Ivr = FALSE; // For running spre early
      WOPT_Enable_Bdce_Before_Ivr = FALSE; // For running bdce early
    }
    
    if (_phase != PREOPT_LNO_PHASE) 
    {
        WOPT_Enable_Loop_Multiver = FALSE;
        WOPT_Enable_Mem_Clear_Remove = FALSE;
        WOPT_Enable_Useless_Store_Elimination = FALSE;
    }

    if (_phase != PREOPT_LNO1_PHASE) 
    {
      WOPT_Enable_Pro_Loop_Fusion_Trans = FALSE;
      WOPT_Enable_Pro_Loop_Interchange_Trans = FALSE;
      WOPT_Enable_Pro_Loop_Ext_Trans = FALSE;
    }
  }

  void Unadjust_Optimization(void) {
    switch (_phase) {
    case PREOPT_PHASE:
      WOPT_Enable_Input_Prop  = _input_prop;  /* For both NVISA and VSA */
      if (Run_vsaopt) {
	WOPT_Enable_Warn_Uninit = _warn_uninit;
	VSA_Aob = _vsa_aob;       /* Array out of bound */
	VSA_Npd = _vsa_npd;       /* Null pointer dereference */
	VSA_Rbc = _vsa_rbc;       /* Rule based check */
	VSA_Uaf = _vsa_uaf;       /* Use after free */
	VSA_Uiv = _vsa_uiv;       /* Uninitialized variable reference */
	VSA_Ral = _vsa_ral;
	VSA_Ddv = _vsa_ddv;
	VSA_Msf = _vsa_msf;
        VSA_Rvsa = _vsa_rvsa;
      }
      break;
    case PREOPT_DUONLY_PHASE:
      /* reset all optimizations */
      WOPT_Enable_Compare_Simp = _compare_simp;
      WOPT_Enable_Add_Do_Loop_Info = _add_do_loop_info;
      WOPT_Enable_Add_Label_Loop_Info = _add_label_loop_info;
      WOPT_Enable_Aggressive_dce = _aggressive_dce;
      WOPT_Enable_Aggressive_Code_Motion = _aggressive_code_motion;
      WOPT_Enable_Alias_Classification = _alias_classification;
      WOPT_Enable_Alias_Class_Fortran_Rule = _alias_class_fortran_rule;
      WOPT_Enable_Copy_Propagate = _copy_propagate;
      WOPT_Enable_CRSIMP         = _crsimp;
      WOPT_Enable_DCE            = _dce;
      WOPT_Enable_DCE_Alias      = _dce_alias;
      WOPT_Enable_DU_Full        = _du_full;
      WOPT_Enable_Edge_Placement = _edge_placement;
      WOPT_Enable_Exp_PRE        = _exp_pre;
      WOPT_Enable_Fold2const     = _fold2const;
      WOPT_Enable_FSA            = _fsa;
      WOPT_Enable_Generate_Trip_Count = _trip;
      WOPT_Enable_Improved_Addr_Taken = _addr;
      WOPT_Enable_Input_Prop     = _input_prop;
      WOPT_Enable_Ivar_Common    = _ivar_common;
      WOPT_Enable_IVE            = _ive;
      WOPT_Enable_IVR            = _ivr;
      WOPT_Enable_Ldx            = _ldx;
      WOPT_Enable_LNO_Copy_Propagate = _lno_copy;
      WOPT_Enable_Load_PRE       = _load_pre;
      WOPT_Enable_Local_Rvi      = _local_rvi;
      WOPT_Enable_Output_Copy    = _ocopy;
      WOPT_Enable_Parm           = _parm;
      WOPT_Enable_Phi_Simp       = _phi_simp;
      WOPT_Enable_RVI            = _rvi;
      WOPT_Enable_Store_PRE      = _store_pre;
      WOPT_Enable_SSA_PRE        = _ssa_pre;
      WOPT_Enable_Verify         = _verify;
      WOPT_Enable_Vsym_Unique    = _vsym_unique;
      OPT_Enable_WHIRL_SSA       = _whirl_ssa;
      WOPT_Enable_WOVP           = _wovp;
      WOPT_Enable_Zero_Version   = _zero_version;
      WOPT_Enable_Epre_Before_Ivr = _epre_before_ivr;
      break;
    case PREOPT_IPA0_PHASE:
    case PREOPT_IPA1_PHASE:
      WOPT_Enable_Compare_Simp = _compare_simp;
      WOPT_Enable_IVR            = _ivr;
      WOPT_Enable_SLT            = _slt;
      OPT_Enable_WHIRL_SSA       = _whirl_ssa;
      WOPT_Enable_WOVP           = _wovp;
      break;
    case MAINOPT_PHASE:
      WOPT_Enable_Compare_Simp = _compare_simp;
      WOPT_Enable_Copy_Prop_Ops_Into_Array = _copy_prop_into_array;
      WOPT_Enable_CRSIMP         = _crsimp;
      WOPT_Enable_DCE_Label      = _dce_label;
      WOPT_Enable_IVR            = _ivr;
      WOPT_Enable_Lego_Opt       = _lego_opt;
      WOPT_Enable_While_Loop     = _while_loop;
      WOPT_Enable_Improved_Addr_Taken = _addr;
      WOPT_Enable_Generate_Trip_Count = _trip;
      WOPT_Enable_LNO_Copy_Propagate  = _lno_copy;
      WOPT_Enable_Zero_Version   = _zero_version;
      OPT_Enable_EH_CFG_OPT = _eh_cfg_opt;
      break;
#ifdef TARG_NVISA
    case PREOPT_CMC_PHASE:
      WOPT_Enable_Goto = _goto;
      WOPT_Enable_Call_Zero_Version = _call_zero_version;
      WOPT_Enable_Zero_Version = _zero_version;
      WOPT_Enable_DU_Full = _du_full;
      break;
#endif
    case PREOPT_LNO1_PHASE:
      WOPT_Enable_SSA_PRE = _ssa_pre;
      WOPT_Enable_Zero_Version = _zero_version;
      WOPT_Enable_IVR = _ivr;
      WOPT_Enable_Copy_Propagate = _lno_copy;
      WOPT_Enable_DCE = _dce;
      WOPT_Enable_Input_Prop = _input_prop;
    case PREOPT_LNO_PHASE:
      if (Run_autopar && Current_LNO->IPA_Enabled) { 
        WOPT_Enable_Call_Zero_Version = _call_zero_version;
        WOPT_Enable_Zero_Version = _zero_version;
        WOPT_Enable_DU_Full = _du_full;
      } 
    default:
      WOPT_Enable_SLT = _slt;
      OPT_Enable_WHIRL_SSA = _whirl_ssa;
      break;
    }

    WOPT_Enable_Pro_Loop_Fusion_Trans = _pro_loop_fusion_trans;
    WOPT_Enable_Pro_Loop_Interchange_Trans = _pro_loop_interchange_trans;
    WOPT_Enable_Pro_Loop_Ext_Trans = _pro_loop_ext_trans;
    WOPT_Enable_Mem_Clear_Remove = _mem_clear_remove;
    WOPT_Enable_Noreturn_Attr_Opt = _no_return;
    WOPT_Enable_Nothrow_Opt = _nothrow;
    WOPT_Enable_Simple_If_Conv = _simp_if_conv;
    WOPT_Enable_Bool_Simp = _bool_simp;
    WOPT_Enable_Bits_Load_Store = _bits_load_store;
    WOPT_Enable_Fold_Lda_Iload_Istore = _fold_lda_iload_istore;
    Enable_WN_Simp = _wn_simp;
    WOPT_Enable_Goto = _goto;
    WOPT_Enable_Combine_Operations = _combine_operations;
    WOPT_Enable_Tail_Recur = _tail_recur;
    WOPT_Enable_Replace_Second_IV = _replace_second_iv;
    WOPT_Enable_Restricted_Map = _restricted_map;
    WOPT_Enable_Value_Numbering = _value_numbering;
    WOPT_Enable_Itself_Prop     = _itself_prop;

    WOPT_Enable_Epre_Before_Ivr = _epre_before_ivr; // For running epre early
    WOPT_Enable_Lpre_Before_Ivr = _lpre_before_ivr; // For running lpre early
    WOPT_Enable_Spre_Before_Ivr = _spre_before_ivr; // For running spre early
    WOPT_Enable_Bdce_Before_Ivr = _bdce_before_ivr; // For running bdce early

    Alias_Pointer_Parms = _alias_pointer_parms;
    WOPT_Enable_Loop_Multiver = _loop_multiver;
    WOPT_Enable_Useless_Store_Elimination = _useless_store_elimination;
  }

public:
  WOPT_SWITCHES(OPT_PHASE phase, INT32 pragma_flags, BOOL disable_parm_alias) {
    _phase = phase;
    _pragma_flags = pragma_flags;

    _alias_pointer_parms = Alias_Pointer_Parms;
    if (disable_parm_alias) 
      Alias_Pointer_Parms = FALSE;

    _add_do_loop_info = WOPT_Enable_Add_Do_Loop_Info;
    _add_label_loop_info = WOPT_Enable_Add_Label_Loop_Info;
    _addr = WOPT_Enable_Improved_Addr_Taken;
    _aggressive_dce = WOPT_Enable_Aggressive_dce;
    _aggressive_code_motion = WOPT_Enable_Aggressive_Code_Motion;
    _alias_classification = WOPT_Enable_Alias_Classification;
    _alias_class_fortran_rule = WOPT_Enable_Alias_Class_Fortran_Rule;
    _bits_load_store = WOPT_Enable_Bits_Load_Store;
    _call_zero_version = WOPT_Enable_Call_Zero_Version;
    _combine_operations = WOPT_Enable_Combine_Operations;
    _compare_simp = WOPT_Enable_Compare_Simp;
    _copy_propagate = WOPT_Enable_Copy_Propagate;
    _copy_prop_into_array = WOPT_Enable_Copy_Prop_Ops_Into_Array;
    _crsimp = WOPT_Enable_CRSIMP;
    _dce = WOPT_Enable_DCE;
    _dce_alias = WOPT_Enable_DCE_Alias;
    _dce_label = WOPT_Enable_DCE_Label;	/* eliminate dead labels? */
    _du_full = WOPT_Enable_DU_Full;	/* full DU for indirects */
    _edge_placement = WOPT_Enable_Edge_Placement;
    _exp_pre = WOPT_Enable_Exp_PRE;
    _fold2const = WOPT_Enable_Fold2const;
    _lno_copy = WOPT_Enable_LNO_Copy_Propagate;
    _fsa = WOPT_Enable_FSA;
    _goto = WOPT_Enable_Goto;
    _input_prop = WOPT_Enable_Input_Prop;
    _itself_prop = WOPT_Enable_Itself_Prop;
    _ivar_common = WOPT_Enable_Ivar_Common;
    _ive = WOPT_Enable_IVE;		/* induction-var elim */
    _ivr = WOPT_Enable_IVR;		/* induction-var recognition */
    _ldx = Indexed_Loads_Allowed;       /* from config.h, -OPT:ldx */
    _lego_opt = WOPT_Enable_Lego_Opt;
    //_ldx = WOPT_Enable_Ldx;
    _load_pre = WOPT_Enable_Load_PRE;
    _local_rvi = WOPT_Enable_Local_Rvi;
    _ocopy = WOPT_Enable_Output_Copy;
    _parm = WOPT_Enable_Parm;		/* put optparm over params */
    _phi_simp = WOPT_Enable_Phi_Simp;
    _rvi = WOPT_Enable_RVI;		/* reg-var identification */
    _slt = WOPT_Enable_SLT;
    _store_pre = WOPT_Enable_Store_PRE;
    _ssa_pre = WOPT_Enable_SSA_PRE;
    _trip = WOPT_Enable_Generate_Trip_Count;
    _update_vsym = WOPT_Enable_Update_Vsym;
    _value_numbering = WOPT_Enable_Value_Numbering;
    _verbose = WOPT_Enable_Verbose;
    _vsa_aob = VSA_Aob;       /* Array out of bound */
    _vsa_npd = VSA_Npd;       /* Null pointer dereference */
    _vsa_rbc = VSA_Rbc;       /* Rule based check */
    _vsa_uaf = VSA_Uaf;       /* Use after free */
    _vsa_uiv = VSA_Uiv;       /* Uninitialized variable reference */
    _vsa_ddv = VSA_Ddv;
    _vsa_ral = VSA_Ral;
    _vsa_msf = VSA_Msf;
    _vsa_rvsa = VSA_Rvsa;
    _vsym_unique = WOPT_Enable_Vsym_Unique;
    _verify = WOPT_Enable_Verify;	/* verify data structures */
    _warn_uninit = WOPT_Enable_Warn_Uninit;
    _while_loop = WOPT_Enable_While_Loop;/*cvt while-do to do-loop */
    _wn_simp = Enable_WN_Simp;
    _whirl_ssa = OPT_Enable_WHIRL_SSA;
    _wovp = WOPT_Enable_WOVP; /* Write-once variable promotion  */
    _zero_version = WOPT_Enable_Zero_Version;
    _vsym_unique = WOPT_Enable_Vsym_Unique;
    _dse_aggressive = WOPT_Enable_Dse_Aggressive;
    _prop_aggressive = WOPT_Enable_Prop_Aggressive;
    _iload_prop = WOPT_Enable_Iload_Prop;
    _vn_full = WOPT_Enable_VN_Full;
    _simp_iload = WOPT_Enable_Simp_Iload;
    _tail_recur = WOPT_Enable_Tail_Recur;
    _replace_second_iv = WOPT_Enable_Replace_Second_IV;
    _restricted_map = WOPT_Enable_Restricted_Map;
    _epre_before_ivr = WOPT_Enable_Epre_Before_Ivr; // For running epre early
    _lpre_before_ivr = WOPT_Enable_Lpre_Before_Ivr; // For running lpre early
    _spre_before_ivr = WOPT_Enable_Spre_Before_Ivr; // For running spre early
    _bdce_before_ivr = WOPT_Enable_Bdce_Before_Ivr; // For running bdce early
    _loop_multiver = WOPT_Enable_Loop_Multiver;
    _useless_store_elimination = WOPT_Enable_Useless_Store_Elimination;
    _pro_loop_fusion_trans = WOPT_Enable_Pro_Loop_Fusion_Trans;
    _pro_loop_interchange_trans = WOPT_Enable_Pro_Loop_Interchange_Trans;
    _pro_loop_ext_trans = WOPT_Enable_Pro_Loop_Ext_Trans;
    _mem_clear_remove = WOPT_Enable_Mem_Clear_Remove;
    _bool_simp = WOPT_Enable_Bool_Simp;
    _fold_lda_iload_istore = WOPT_Enable_Fold_Lda_Iload_Istore;
    _no_return = WOPT_Enable_Noreturn_Attr_Opt;
    _nothrow = WOPT_Enable_Nothrow_Opt;
    _simp_if_conv = WOPT_Enable_Simple_If_Conv;
    _eh_cfg_opt = OPT_Enable_EH_CFG_OPT;

    Adjust_Optimization();
  }
  ~WOPT_SWITCHES(void)                       { Unadjust_Optimization(); }

};

//  COMP_UNIT constructor
//
COMP_UNIT::COMP_UNIT(WN *t, ALIAS_MANAGER *am, OPT_PHASE phase, 
		     MEM_POOL *gpool,MEM_POOL *lpool)
{
  // extern void  Initialize_CR_simp(CODEMAP*); // or we include opt_fold.h

  _phase = phase;
  Set_tlog_phase(phase);
  _mem_pool = gpool;
  _loc_pool = lpool;
  _input_tree = t;
  _rid = REGION_get_rid(_input_tree);
  _arule = am->Rule();
  _alias_mgr = am;
  _cfg = CXX_NEW(CFG(gpool, lpool), gpool);
  _opt_stab = CXX_NEW(OPT_STAB(gpool), gpool);
  _exc = CXX_NEW(EXC(_cfg, _opt_stab, gpool), gpool); 
  _cfg->Set_exc(_exc);
  _ssa = CXX_NEW(SSA(gpool, lpool), gpool);
  _emitter =  CXX_NEW(EMITTER(lpool, gpool, phase), gpool);
  _htable = CXX_NEW(CODEMAP(CODE_HTABLE_SIZE, _cfg,
			    _opt_stab, _ssa,
                            VAR_PHI_HASH_SIZE, phase, gpool),
		    gpool);
#ifdef BUILD_MASTIFF
  _cda = NULL;
  _vra = NULL;
  _vsa = NULL;
  _dna = NULL;
  _eh_table = NULL;
#endif
  _main_emitter = NULL;
  WN_init_flags(gpool);            // Create a debugging WN map
  Initialize_CR_simp(_htable);     // CR simplifier needs to be initialized
  VN_EXPR::Init_Free_Lists(gpool); // Initialize free lists for value numbering

}


// COMP_UNIT destructor
//
COMP_UNIT::~COMP_UNIT(void)
{
  VN_EXPR::Reclaim_Free_Lists(); // Reclaim free lists for value numbering
  WN_fini_flags();                // Delete the debugging WN map
#ifdef BUILD_MASTIFF
  CXX_DELETE(_eh_table, _mem_pool);
  CXX_DELETE(_cda,      _mem_pool);
  CXX_DELETE(_vra,      _mem_pool);
#endif
  CXX_DELETE(_cfg,      _mem_pool);
  CXX_DELETE(_opt_stab, _mem_pool);
  CXX_DELETE(_ssa,      _mem_pool);
  CXX_DELETE(_emitter,  _mem_pool);
  CXX_DELETE(_htable,   _mem_pool);
  CXX_DELETE(_exc,      _mem_pool);
}


void COMP_UNIT::Create_mainopt_res(void)
{ 
  _cfg->Analyze_loops();
}


// Return TRUE if optimization should be skipped
// NOTE: this is used by Perform_Global_Optimization (wodriver.c) and by
// Pre_Optimizer (opt_main.cxx).
// NOTE: WN_st is not defined for a region.
static BOOL Disable_opt(WN *wn_tree, ST *pu_st)
{
  BOOL is_pu = (WN_opcode(wn_tree) == OPC_FUNC_ENTRY);
  char *pu_name = ST_name(pu_st);
  char *rgn_name = (char*)alloca(strlen(pu_name) + 32);

  if (is_pu)
    sprintf(rgn_name,"function %s",pu_name);
  else
    sprintf(rgn_name,"region %d (function %s)",
	    RID_id(REGION_get_rid(wn_tree)),pu_name);

#ifndef __MINGW32__
  // skip the functions specified
  if (WOPT_Enable_Skip != NULL) {
    regex_t buf;
    regmatch_t dummy[1];
    if (regcomp(&buf, WOPT_Enable_Skip, REG_NOSUB | REG_NEWLINE) == 0 &&
        regexec(&buf, pu_name, 1, dummy, 0) == 0) {
      DevWarn("WOPT skip %s", rgn_name);
      return TRUE;
    }
  }
#endif /* __MINGW32__ */

  // skip_equal, skip_before, skip_after function count specified
  if ( Query_Skiplist ( WOPT_Skip_List, Current_PU_Count() ) ) {
    if ( Show_Progress )
      ErrMsg(EC_Skip_PU, " WOPT", Current_PU_Count(), rgn_name);
    return TRUE;
  }

#ifndef __MINGW32__
  // process the functions specified
  if (WOPT_Enable_Process != NULL) {
    regex_t buf;
    regmatch_t dummy[1];
    if (regcomp(&buf, WOPT_Enable_Process, REG_NOSUB | REG_NEWLINE) == 0 &&
        regexec(&buf, pu_name, 1, dummy, 0) == 0) {
    } else {
      DevWarn("WOPT skip %s", rgn_name);
      return TRUE;
    }
  }
#endif /* __MINGW32__ */

  return FALSE;
}

static BOOL
This_preopt_renumbers_pregs(INT32 phase)
{
  return ((((phase == PREOPT_IPA0_PHASE) && // ipl
	    IPA_Enable_Alias_Class) ||
	   (phase == PREOPT_LNO_PHASE) ||
	   (phase == PREOPT_PHASE)) &&
	  !Is_Set_PU_Info_flags(Current_PU_Info,
				PU_PREGS_RENUMBERED));
}


static void
Verify_Codemap(COMP_UNIT *comp_unit)
{
#ifdef Is_True_On  
  SET_OPT_PHASE("Verify CODEMAP");

  Is_True(comp_unit->Verify_IR(comp_unit->Cfg(),comp_unit->Htable(),0),
	  ("Verify CFG wrong"));

  if (Get_Trace(TKIND_INFO, TINFO_TIME)) 
    SET_OPT_PHASE("Skip verify CODEMAP because timing trace is on");
  else
    Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));

  if (Get_Trace(TKIND_INFO, TINFO_TIME)) 
    SET_OPT_PHASE("Skip verify CODEMAP because timing trace is on");
  else {
    Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));
    comp_unit->Htable()->Verify_hashing();
  }
  
  if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
    SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
  } else {
    SET_OPT_PHASE("Verify Live-Range");
    comp_unit->Verify_version();
  }
#endif // Is_True_On  
} // Verify_Codemap


extern void CFG_transformation(COMP_UNIT *cu, bool, bool, bool);
extern void Rename_CODEMAP(COMP_UNIT *);


static void 
Do_Pre_Before_Ivr(COMP_UNIT *comp_unit)
{
  // This provides an experimental framework for moving PRE and BCDE up
  // before IVR, and is off by default.  It has been minimally tested, but
  // works for most of the cases on which it has been tested.  One problem
  // encountered, which has not yet been fixed, is the following assertion
  // when compiling sanity_tests/overall/compile_only/cmetric.c with options
  // "-n32 -mips4 -O3 -c -WOPT:lpre4ivr=on:spre4ivr=on:bdce4ivr=on":
  //
  //   ### Assertion failure at line 706 of ../../be/opt/opt_htable.h:
  //   ### Compiler Error in file vn26_cmetric.c during Global Optimization
  //       -- New PRE: Build initial occurrence lists phase:
  //   ### CODEREP::Kid_count, illegal kind CK_VAR
  //

  // Always do a minimal DCE before early versions of PRE and BDCE,
  // since it sets up the and marks PHI nodes (liveness) appropriately
  // for these phases.
  //
  if (WOPT_Enable_Epre_Before_Ivr ||
      WOPT_Enable_Lpre_Before_Ivr ||
      WOPT_Enable_Spre_Before_Ivr ||
      WOPT_Enable_Bdce_Before_Ivr)
  {
    BOOL dummy;
    comp_unit->Do_dead_code_elim(FALSE/*do_unreachable*/,
				 FALSE/*do_dce_global*/,
				 FALSE/*do_dce_alias*/,
				 FALSE/*do_agg_dce*/,
				 FALSE/*do_identity_removal*/,
				 FALSE/*do_preg_renumbering*/,
				 &dummy/*paths_removed*/);
  }

  // Disable strength reduction, lftr, and pre for constants in this
  // early invocation of PRE algorithms.
  //

  if (WOPT_Enable_Epre_Before_Ivr)
  {
        SET_OPT_PHASE("SSA PRE before RVI");
        comp_unit->Do_new_pre();
        if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
          comp_unit->Htable()->Print(TFile);
        }
	Verify_Codemap(comp_unit);
  }
  if (WOPT_Enable_Lpre_Before_Ivr)
  {
    SET_OPT_PHASE("Load PRE before RVI");
    comp_unit->Htable()->Verify_var_phi_hash();
    comp_unit->Do_load_pre(FALSE /*do_consts*/, TRUE /*do_loads*/);
    
    if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
      comp_unit->Htable()->Print(TFile);
    }
    Verify_Codemap(comp_unit);
  }
  if (WOPT_Enable_Spre_Before_Ivr)
  {
    SET_OPT_PHASE("Store PRE before RVI");
    comp_unit->Htable()->Verify_var_phi_hash();
    comp_unit->Do_store_pre();

    if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
      comp_unit->Htable()->Print(TFile);
    }
    Verify_Codemap(comp_unit);
  }
  if (WOPT_Enable_Bdce_Before_Ivr)
  {
    SET_OPT_PHASE("Bitwise DCE before RVI");
    comp_unit->Do_bitwise_dce(FALSE /* copy propatage on */);
  }

  if (WOPT_Enable_Epre_Before_Ivr || WOPT_Enable_Epre_Before_Ivr)
  {
    SET_OPT_PHASE("Second rename before RVI");
    Rename_CODEMAP(comp_unit);
  }

} // Do_Pre_Before_Ivr


WN *
Pre_Optimizer(OPT_PHASE phase, WN *wn_tree, DU_MANAGER *du_mgr,
	      ALIAS_MANAGER *alias_mgr)
{
  WN *wn_orig = wn_tree; // needed for region <--> RID consistency

  SET_OPT_PHASE("Preparation");

  if (Get_Trace(TP_GLOBOPT, -1)) {
    fprintf (TFile,  "%s \t Pre_Optimizer phase=%d\n%s\n", DBar, phase, DBar);
  }

  Is_True(phase == PREOPT_IPA0_PHASE ||
	  phase == PREOPT_IPA1_PHASE ||
	  phase == PREOPT_LNO_PHASE ||
	  phase == PREOPT_LNO1_PHASE ||
	  phase == PREOPT_DUONLY_PHASE ||
	  phase == PREOPT_PHASE ||
	  phase == MAINOPT_PHASE,
          ("Unknown optimizer phase."));

  Is_True(WN_opcode(wn_orig)==OPC_FUNC_ENTRY || WN_opcode(wn_orig)==OPC_REGION,
	  ("Pre_Optimizer, unknown WHIRL entry point"));

  // sets Opt_current_pu_st static
  Opt_set_current_pu_name(wn_tree);

  // If dont_opt is TRUE then do lowering only.
  BOOL dont_opt = Disable_opt(wn_tree, Opt_current_pu_st);

  WN *wopt_pragma = (WN_opcode(wn_orig) == OPC_FUNC_ENTRY) ? 
    WN_func_pragmas(wn_orig) : WN_region_pragmas(wn_orig);
  INT32 pragma_flags = 0;
  BOOL  disable_parm_alias = FALSE;
  for (wopt_pragma = WN_first(wopt_pragma);
       wopt_pragma != NULL;
       wopt_pragma = WN_next(wopt_pragma)) {
	 if ( WN_pragma(wopt_pragma) == WN_PRAGMA_WOPT_FINISHED_OPT ) {
	   pragma_flags = WN_pragma_arg2(wopt_pragma);
	 }
       }

  if (IS_FORTRAN && PU_args_aliased(Pu_Table[Opt_current_pu])) {
    disable_parm_alias = TRUE;
  }

  WOPT_SWITCHES WOPT_Enable(phase, pragma_flags, disable_parm_alias);

  // A nested function that is not MP does not inherit the 
  // restricted mapping. e.g. varfmt nested functions.
  if (WN_opcode(wn_orig) != OPC_REGION) {
    if (PU_is_nested_func(Pu_Table[Opt_current_pu]) &&
	!PU_mp(Get_Current_PU()))
      WOPT_Enable_Restricted_Map = FALSE;
  }

  BOOL Fold_ILOAD_save = WN_Simp_Fold_ILOAD;
  BOOL Fold_LDA_save = WN_Simp_Fold_LDA;

  enable_tree_freq_display();  // enable frequency display for ascii
			       // WHIRL dumps

  if (phase == MAINOPT_PHASE) {
    SET_OPT_PHASE("Mainopt Lowering");
    WN_Simplifier_Enable(TRUE);
    // check if last time through PU, if so lower region exits
    INT32 lower_region_exits_flag = ((PU_has_region(Get_Current_PU()) &&
				      WN_opcode(wn_orig) == OPC_FUNC_ENTRY) ?
				     LOWER_REGION_EXITS : 0);
    WN_Simp_Fold_ILOAD = TRUE;
    WN_Simp_Fold_LDA = TRUE;
    LOWER_ACTIONS actions =
	LOWER_COMPLEX |
	LOWER_BASE_INDEX |
	LOWER_ARRAY |
	LOWER_ALL_MAPS |
	LOWER_INLINE_INTRINSIC |
	LOWER_IO_STATEMENT |
	LOWER_ENTRY_EXIT |
	LOWER_SHORTCIRCUIT |
	lower_region_exits_flag;	// this is a variable

#ifdef KEY
    // No uplevel reference spliting for openmp
    if ((PU_has_mp (Get_Current_PU ()) == FALSE) && 
        (PU_mp(Get_Current_PU ()) == FALSE))
      actions |= LOWER_UPLEVEL;
#endif
    
    if (WOPT_Enable_Bits_Load_Store)
	actions |= LOWER_BIT_FIELD_ID;
    else
	actions |= LOWER_BITS_OP;

    if (WOPT_Simplify_Bit_Op)
      actions |= LOWER_SIMPLIFY_BIT_OP;
                                                                                                                                                             
    actions |= LOWER_TO_MEMLIB; // add memlib transformation
 
    wn_tree = WN_Lower(wn_orig, actions, alias_mgr, "Pre_Opt");

#if defined(TARG_X8664) || defined(TARG_NVISA) || defined(TARG_LOONGSON)
    BOOL target_64bit = Is_Target_64bit();
#elif defined(TARG_SL) || defined(TARG_PPC32)
    BOOL target_64bit = FALSE;
#else
    BOOL target_64bit = TRUE;
#endif

#ifdef KEY
    if (target_64bit && WOPT_Enable_Retype_Expr)
      WN_retype_expr(wn_tree);
#endif

#if defined(KEY) && !(defined(TARG_IA64) || defined(TARG_SL))
    if (WOPT_Enable_Innerloop_Unroll)
      WN_unroll(wn_tree);
#endif

    if (Cur_PU_Feedback)
      Cur_PU_Feedback->Reset_Root_WN(wn_tree);
    if (Only_Unsigned_64_Bit_Ops && ! Delay_U64_Lowering)
      U64_lower_wn(wn_tree, FALSE); 	 // lowering for unsigned 64-bit ISA
    Is_True(REGION_consistency_check(wn_tree),
	    ("Pre_Optimizer REGION inconsistency")); // lowerer maintains RID
    wn_orig = wn_tree; // new orig tree

    // return if we should not optimize this function
    if (dont_opt) return wn_tree;

#if defined(BUILD_MASTIFF) && defined(XVSA_PROTECT_NONE)
    if (time(NULL) - VSA_BUILD_DATE >= 60 * 60 * 24 * 120) return wn_tree;  // expire in 120 days
#endif

    WN_Simplifier_Enable(FALSE);
    
    if ( Get_Trace( TP_GLOBOPT, OPT_LOWER_FLAG ) ) {
      fprintf( TFile, "%sAfter Mainopt Lowering\n%s",DBar,DBar );
      fdump_tree(TFile, wn_tree);
    }
    
    if (phase == MAINOPT_PHASE && Current_Dep_Graph != NULL &&
	Get_Trace(TP_LNOPT, 1)) {
      /* Trace LNO graph for CG again, since WN addresses have changed */
      fprintf(TFile, "%sLNO dep graph for CG, before WOPT\n%s", DBar, DBar);
      Current_Dep_Graph->Print(TFile);
      fprintf(TFile, "%s", DBar);
    }
  } // if (phase == MAINOPT_PHASE)
  else {
    if ( Get_Trace( TP_GLOBOPT, OPT_LOWER_FLAG ) ) {
      fprintf( TFile, "%sAfter Vho Lowering\n%s",DBar,DBar );
      fdump_tree(TFile, wn_tree);
    }

    // Workaround for 440589 for v7.1.   Lower IO-statements in preopt if
    // the PU contains NAMELIST because the NAMELIST side-effect
    // is only represented in the lowered IOs.

    LOWER_ACTIONS lower_flags = LOWER_NULL;

    if (PU_has_namelist(Get_Current_PU())) 
      lower_flags |= LOWER_IO_STATEMENT;

    if (WOPT_Enable_Lower_Short_Circuit ||
	(Cur_PU_Feedback && !WOPT_Enable_Lower_Short_Circuit_Set)) 
      lower_flags |= LOWER_SHORTCIRCUIT;

    if (! WOPT_Enable_Bits_Load_Store)
      lower_flags |= LOWER_BITS_OP;
    
    if (lower_flags != LOWER_NULL) {
      wn_tree = WN_Lower(wn_tree, lower_flags, alias_mgr, 
			 "Pre_opt: special lowering for NAMELIST");
      if (Cur_PU_Feedback)
	Cur_PU_Feedback->Reset_Root_WN(wn_tree);
    }

    // return if we should not optimize this function
    if (dont_opt) return wn_tree;

  }

  Init_opt_memory_pools();

  // goto conversion
  if (WOPT_Enable_Goto &&
      (phase == PREOPT_LNO_PHASE 
       || phase == PREOPT_PHASE
#ifdef TARG_NVISA
       || phase == PREOPT_CMC_PHASE
#endif
       )) {
#ifdef KEY
    // goto_skip_equal, goto_skip_before, goto_skip_after PU count specified
    if ( Query_Skiplist ( Goto_Skip_List, Current_PU_Count() ) ) {
      if ( Show_Progress )
        ErrMsg(EC_Skip_PU, " goto conversion", Current_PU_Count(), Cur_PU_Name);
    }
    else {
#endif
    SET_OPT_PHASE("Goto conversion");
    OPT_POOL_Push( &Opt_local_pool, MEM_DUMP_FLAG+2 );
    {
      GOTO_TABLE goto_table( wn_tree, &Opt_local_pool );
      goto_table.Remove_Gotos();
      // goto_table gets destructed before we pop the pool
    }
    OPT_POOL_Pop( &Opt_local_pool, MEM_DUMP_FLAG+2 );

    if ( Get_Trace( TP_GLOBOPT, OPT_LOWER_FLAG ) ) {
      fprintf( TFile, "%sAfter Goto Conversion\n%s",DBar,DBar );
      fdump_tree(TFile, wn_tree);
    }
#ifdef KEY
    }
#endif
  }

  if (phase == PREOPT_IPA0_PHASE) // ipl
  {
    identify_complete_struct_relayout_candidates(wn_tree);
    identify_array_remapping_candidates(wn_tree, NULL);
  }

  SET_OPT_PHASE("Preparation");

#ifdef SKIP
  // check for inadvertent increase in size of data structures
  Is_True(sizeof(CODEREP) == 48,
    ("Size of CODEREP has been changed (is now %d)!",sizeof(CODEREP)));
#if defined(linux) || defined(BUILD_OS_DARWIN)
  Is_True(sizeof(STMTREP) == 60,
    ("Size of STMTREP has been changed (is now %d)!",sizeof(STMTREP)));
#else
  Is_True(sizeof(STMTREP) == 64,
    ("Size of STMTREP has been changed (is now %d)!",sizeof(STMTREP)));
#endif
#endif

  // allocate space for cfg, htable, and itable
  MEM_POOL* gpool;
  if (phase == PREOPT_PHASE) {
    gpool = &Opt_preopt_pool;
    OPT_POOL_Push(gpool, MEM_DUMP_FLAG+1);
  }
  else {
    gpool = &Opt_global_pool;
  }
  COMP_UNIT *comp_unit = CXX_NEW(COMP_UNIT(wn_tree, alias_mgr,
		                 phase, gpool, &Opt_local_pool),
				 gpool);

//#ifdef Is_True_On
  g_comp_unit = comp_unit;
  // comp_unit->Verify_addr_taken();
//#endif

  Is_True(comp_unit->Cfg()->Verify_tree(comp_unit->Input_tree()), 
	  ("Verifying CFG wrong"));

  // these flags tell the symbol table and cfg what to do
  BOOL lower_fully = (phase == MAINOPT_PHASE);
  // if lower_fully, we are definitely in mainopt
  // if we are preopt, need to see who caller is
  REGION_LEVEL rgn_level = (lower_fully) ? RL_MAINOPT :
  					   RID_preopt_level(phase);

  // create aux symbol table
  // cannot print WHIRL tree after this point, use dump_tree_no_st
  SET_OPT_PHASE("Create AUX Symbol table");
  WN_Simplifier_Enable(TRUE);	// so that I can fold ILOAD-LDA
  comp_unit->Opt_stab()->Create(comp_unit, rgn_level);
  if (WOPT_Enable_Pt_Summary) {
    comp_unit->Opt_stab()->Points_to_summarizer()->
       Bind_callee_points_to_summary (wn_tree);
  }

  MEM_POOL alias_class_pool;

  OPT_POOL_Initialize(&alias_class_pool, "Alias classification pool",
		      FALSE, MEM_DUMP_FLAG+20);

  ALIAS_CLASSIFICATION ac(comp_unit->Opt_stab(),
			  AC_DESTINATION_OPT_STAB,
			  &alias_class_pool);

  comp_unit->Opt_stab()->Set_alias_classification(ac);

  // Alias classification computation should move from here so that the
  // alias class finalization process can update the OCC_TAB_ENTRY for
  // each indirect memop. See comment below, after Compute_FFA().
  if (WOPT_Enable_Alias_Classification &&
      !REGION_has_black_regions(comp_unit->Rid())) {
    SET_OPT_PHASE("Compute alias classification");
    ac.Classify_memops(comp_unit->Input_tree());
    comp_unit->Opt_stab()->Incorporate_alias_class_info();
  }

#ifdef BUILD_MASTIFF
  if (phase == PREOPT_PHASE && Run_vsaopt ) {
    SET_OPT_PHASE("Create DNA_NODE");
    // just to create DNA_NODE
    // and initialize INLCXT_MAP and STPATH_MAP
    // For INLCXT_MAP, need to do this before create CFG
    comp_unit->Do_vsa(IPSA_manager);
  }
#endif

#ifdef Is_True_On
  g_opt_stab = comp_unit->Opt_stab();
#endif
  WN_Simplifier_Enable(FALSE);
  WN_Simp_Fold_ILOAD = Fold_ILOAD_save;;
  WN_Simp_Fold_LDA = Fold_LDA_save;;

  // create control flow graph
  SET_OPT_PHASE("Create CFG");

  // tail recursion
  BOOL do_tail_rec = ((! IS_FORTRAN) && WOPT_Enable_Tail_Recur);

  comp_unit->Cfg()->Create(comp_unit->Input_tree(), lower_fully,
			   WOPT_Enable_Calls_Break_BB,
			   rgn_level/*context*/, 
			   comp_unit->Opt_stab(), do_tail_rec,
			   Malloc_Mem_Pool);

  // Transfer feedback data from Whirl annotation (at Cur_PU_Feedback)
  // to optimizer CFG annotation (at comp_unit->Cfg()->Feedback())
  if (Cur_PU_Feedback) {
    SET_OPT_PHASE("Annotate CFG with feedback from Whirl");
    OPT_FEEDBACK *feedback = CXX_NEW(OPT_FEEDBACK(comp_unit->Cfg(),
						  &Opt_global_pool),
				     &Opt_global_pool);
    comp_unit->Cfg()->Set_feedback( feedback );
    comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					  "after CFG Annotation" );
    // TODO: Perhaps clear out Cur_PU_Feedback now?
  }

  SET_OPT_PHASE("Control Flow Analysis");
  comp_unit->Cfg()->Compute_dom_tree(TRUE); // create dominator tree
  comp_unit->Cfg()->Compute_dom_tree(FALSE); // create post-dominator tree
  comp_unit->Cfg()->Remove_fake_entryexit_arcs();
  comp_unit->Cfg()->Compute_dom_frontier(); // create dominance frontier
  comp_unit->Cfg()->Compute_control_dependence(); // create control-dependence set

  SET_OPT_PHASE("Proactive Loop Transformation");
  comp_unit->Pro_loop_trans();
  comp_unit->Cfg()->Analyze_loops();

  // Setup flow free alias information  --  CHI and MU list 
  SET_OPT_PHASE("Create MU and CHI list");
  comp_unit->Opt_stab()->Compute_FFA(comp_unit->Rid());

  // Now the OCC_TAB_ENTRY for each indirect memop is set up, and the
  // correspondence between memops and POINTS_TO's is finalized. We
  // should delay alias classification until now so it can fill in the
  // _alias_class field of each POINTS_TO as it finalizes the
  // equivalence class for each memop.
  //
  // For the sake of not changing too much at once, though, I'm
  // leaving it above for now.

  Is_True(comp_unit->Cfg()->Verify_cfg(),
	  ("Verify CFG wrong after MU and CHI"));

  SET_OPT_PHASE("Create SSA Representation");
  // create ssa representation
  comp_unit->Ssa()->Construct(comp_unit->Htable(),
			      comp_unit->Cfg(),
			      comp_unit->Opt_stab());
  // redundancy elimination with reassociation 

  if (Get_Trace(TP_GLOBOPT, SSA_DUMP_FLAG)) {
    fprintf(TFile, "\nAfter SSA Construction...\n");
    comp_unit->Ssa()->Print(TFile);
  }

  // Why do we wait until now to free the alias class resources? It
  // seems to me we could do it after
  // comp_unit->Opt_stab()->Compute_FFA()
  // above. -- RK 980722
  //
  // Note that this line deletes the alias classification memory pool.
  comp_unit->Opt_stab()->Alias_classification()->Release_resources();
  if (WOPT_Enable_Pt_Summary) {
    SET_OPT_PHASE("Points-to Summary Annotation");
    comp_unit->Opt_stab()->Points_to_summarizer()->
       Annotate_points_to_summary();
  }

  SET_OPT_PHASE("SSA Pointer Alias Analysis");
  comp_unit->Ssa()->Pointer_Alias_Analysis();
  SET_OPT_PHASE("Dead Store Elimination");
  comp_unit->Ssa()->Dead_store_elim(comp_unit);
  if (phase == MAINOPT_PHASE) {
    SET_OPT_PHASE("Reassociation enabled CSE");
    comp_unit->Do_reasso();
  }
  comp_unit->Opt_stab()->Update_return_mu();
  Analyze_pu_attr (comp_unit->Opt_stab(), Opt_current_pu_st);
  
  if (WOPT_Enable_Zero_Version) {
    SET_OPT_PHASE("Find Zero Versions");
    comp_unit->Ssa()->Find_zero_versions();
  }

  SET_OPT_PHASE("Create CODEMAP Representation");
  comp_unit->Ssa()->Create_CODEMAP();

  if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
    SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
  } else {
    SET_OPT_PHASE("Verify Live-Range");
    comp_unit->Verify_version();
  }

  SET_OPT_PHASE("Verify DO-loop");
  Detect_invalid_doloops(comp_unit);

  Is_True(comp_unit->Verify_IR(comp_unit->Cfg(), comp_unit->Htable(), 1),
	  ("Verify CFG wrong after Htable"));

#ifdef TARG_NVISA
  // want to do partial preopt for unrolling, but not full preopt
  // (need to create cfg for unroller, and codemap for emitter).
  if (phase == MAINOPT_PHASE || phase == PREOPT_CMC_PHASE) {
#endif

  // Do some redundancy elimination phases early, to expose second order
  // effects and deal with them in the subsequent phases (e.g. CVTLs).
  //
  Do_Pre_Before_Ivr(comp_unit);

  // do induction variable recognition
  if (WOPT_Enable_IVR) {
    SET_OPT_PHASE("Induction Variable Recognition");
    comp_unit->Do_iv_recognition();
  }

#if 0
  // do vulnerability scan
  if (Run_vsaopt) {
    SET_OPT_PHASE("Vulnerability Static Analysis");
    comp_unit->Do_vsa(IPSA_manager);
  }
#endif
  // do flow free copy propagation
  if (WOPT_Enable_Copy_Propagate) {
    SET_OPT_PHASE("Copy Propagation");
    comp_unit->Do_copy_propagate();
  }

  if (WOPT_Enable_Bool_Simp) {
    SET_OPT_PHASE("Boolean simplification");
    Simplify_bool_expr(comp_unit); 
  }

  Is_True(comp_unit->Verify_IR(comp_unit->Cfg(), comp_unit->Htable(), 2),
	  ("Verify CFG wrong after copy prop"));

  // do dead-code elimination (both unreachable and dead-stores)
  if (WOPT_Enable_DCE) {
    SET_OPT_PHASE("Dead Code Elimination");
    BOOL dce_renumber_pregs = This_preopt_renumbers_pregs(phase);
    comp_unit->Do_dead_code_elim(TRUE, TRUE, TRUE, TRUE,
				 WOPT_Enable_Copy_Propagate,
				 dce_renumber_pregs,
				 NULL);

    if ( comp_unit->Cfg()->Feedback() )
	 comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					       "after Dead Code Elimination" );
  }

#ifdef KEY
  if (WOPT_Enable_Warn_Uninit) // just wanted to gurantee run once with this funciton; && phase == MAINOPT_PHASE)
    comp_unit->Find_uninitialized_locals();
#endif

#ifdef KEY // moved here because renaming causes bad code when there is
    	   // overlapped live ranges, which can be created by copy propagation
  if ( WOPT_Enable_Fold_Lda_Iload_Istore ) {
    SET_OPT_PHASE("LDA-ILOAD/ISTORE folding in coderep");
    comp_unit->Fold_lda_iload_istore();
  }
#endif

  if (phase != PREOPT_LNO1_PHASE) {
    for (INT i = 0; i < WOPT_Enable_Extra_Rename_Pass; ++i) {

      if (Get_Trace(TP_WOPT2, SECOND_RENAME_FLAG)) 
	fprintf(TFile, "%sEXTRA RENAME PASS %d:\n%s", DBar, i+1, DBar);

      // only enable during MAINOPT_PHASE because the update of high level
      // structure is not implemented.  -Raymond 5/29/98.
      //
      if (WOPT_Enable_CFG_Opt && phase == MAINOPT_PHASE) {
	SET_OPT_PHASE("CFG optimization");
	CFG_transformation(comp_unit,
			   WOPT_Enable_CFG_Opt2 && i == 0, // first pass
			   Get_Trace(TP_WOPT2, CFG_OPT_FLAG),
			   WOPT_Enable_CFG_Display);

	if ( comp_unit->Cfg()->Feedback() )
	  comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
						"after CFG Optimization" );
      }

      SET_OPT_PHASE("Second rename");
      Rename_CODEMAP(comp_unit);

      if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
	SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
      } else {
	SET_OPT_PHASE("Verify Live-Range");
	comp_unit->Verify_version();
      }

      // do flow free copy propagation
      if (WOPT_Enable_Copy_Propagate) {
	SET_OPT_PHASE("Copy Propagation");
	comp_unit->Do_copy_propagate();
      }

      if (WOPT_Enable_DCE) {
	SET_OPT_PHASE("Dead Code Elimination");
	BOOL paths_removed;
	BOOL dce_renumber_pregs = This_preopt_renumbers_pregs(phase);
	comp_unit->Do_dead_code_elim(TRUE, TRUE, TRUE, TRUE,
				     WOPT_Enable_Copy_Propagate,
				     dce_renumber_pregs,
				     &paths_removed);

	if ( comp_unit->Cfg()->Feedback() )
	  comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
						"Dead Code Elimination" );
      
	if (!paths_removed) break;
    }

#ifdef KEY // moved here because renaming causes bad code when there is
    	   // overlapped live ranges, which can be created by copy propagation
      if ( WOPT_Enable_Fold_Lda_Iload_Istore ) {
	SET_OPT_PHASE("LDA-ILOAD/ISTORE folding in coderep");
	comp_unit->Fold_lda_iload_istore();
      }
#endif

      // synchronize CFG and feedback info
      // comp_unit->Cfg()->Feedback().make_coherent();

      if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
	SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
      } else {
	SET_OPT_PHASE("Verify Live-Range");
	comp_unit->Verify_version();
      }
    }
  }

  Is_True(comp_unit->Verify_IR(comp_unit->Cfg(), comp_unit->Htable(), 3),
	  ("Verify CFG wrong after extra passes"));

#ifdef BUILD_MASTIFF
  // do vulnerability scan
  if (phase == MAINOPT_PHASE && Run_vsaopt) {
    SET_OPT_PHASE("Vulnerability Static Analysis");
    comp_unit->Do_vsa(IPSA_manager);
  }
#endif

  if (WOPT_Enable_Edge_Placement && phase == MAINOPT_PHASE) {
    SET_OPT_PHASE("Remove Critical Edge");
    Is_Trace( Get_Trace(TP_GLOBOPT, EPRE_DUMP_FLAG), 
	     ( TFile, "-------CFG before edge placement---------\n" ) );
    Is_Trace_cmd( Get_Trace(TP_GLOBOPT, EPRE_DUMP_FLAG), 
		 comp_unit->Cfg()->Print(TFile) );
    INT count = comp_unit->Cfg()->Remove_critical_edge();
    Is_Trace( Get_Trace(TP_GLOBOPT, EPRE_DUMP_FLAG), 
	     ( TFile, "After edge Placement: BBs are placed on %d edges\n",
	       count ) );
    Is_Trace_cmd( (count>0 && Get_Trace(TP_GLOBOPT, EPRE_DUMP_FLAG)), 
		  comp_unit->Cfg()->Print(TFile) );

    if ( comp_unit->Cfg()->Feedback() )
      comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					    "after Edge Placement" );
  }

  if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
    SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
  } else {
    SET_OPT_PHASE("Verify Live-Range");
    comp_unit->Verify_version();
  }

#ifdef Is_True_On  
  SET_OPT_PHASE("Verify CODEMAP");
  Is_True(comp_unit->Verify_IR(comp_unit->Cfg(),comp_unit->Htable(),4),
	  ("Verify CFG wrong after dce"));
  if (Get_Trace(TKIND_INFO, TINFO_TIME)) 
    SET_OPT_PHASE("Skip verify CODEMAP because timing trace is on");
  else {
    Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));
    comp_unit->Htable()->Verify_hashing();
  }
#endif // Is_True_On

  if(WOPT_Enable_WOVP){
     WOVP wovp(comp_unit->Cfg(), comp_unit->Opt_stab());
     wovp.Do_wovp();
  }

#ifdef TARG_NVISA
  } // MAINOPT_PHASE
#endif

  // If this is the optimizer phase, we have more work to do
  WN *opt_wn;
  if ( phase == MAINOPT_PHASE ) {
#if defined(TARG_IA64) || defined(TARG_NVISA)
    if (WHIRL_Mtype_B_On)
      comp_unit->Introduce_mtype_bool();
#endif

    if (WOPT_Enable_Bits_Load_Store) {
      SET_OPT_PHASE("{I,}{LD,ST}BITS lowering in coderep");
      comp_unit->Lower_to_extract_compose();
      // so that LPRE/SPRE will only work on scalars, not bit-fields
    }

    if (WOPT_Enable_SSA_PRE) {
      if (WOPT_Enable_Exp_PRE) {

	if ( phase == MAINOPT_PHASE ) {
	  comp_unit->Do_vnfre(TRUE/*before_epre*/);
	}

        SET_OPT_PHASE("SSA PRE");
        comp_unit->Do_new_pre();

        if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
          fprintf(TFile, "%sAfter Do_new_pre\n%s", DBar, DBar);
          comp_unit->Htable()->Print(TFile);
        }

	comp_unit->Do_vnfre(FALSE/*before_epre*/);

#ifdef Is_True_On  
        SET_OPT_PHASE("Verify CODEMAP");
        Is_True(comp_unit->Verify_IR(comp_unit->Cfg(),comp_unit->Htable(),5),
                ("Verify CFG wrong after dce"));
        if (Get_Trace(TKIND_INFO, TINFO_TIME)) 
          SET_OPT_PHASE("Skip verify CODEMAP because timing trace is on");
        else
          Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));
#endif // Is_True_On
        if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
          SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
        } else {
          SET_OPT_PHASE("Verify Live-Range");
          comp_unit->Verify_version();
        }
      }

      if (WOPT_Enable_DCE) {
	SET_OPT_PHASE("Dead Code Elimination 2");
	comp_unit->Do_dead_code_elim(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,
				     NULL);

	if ( comp_unit->Cfg()->Feedback() )
	  comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
						"after Dead Code"
						" Elimination 2" );
      }
	
      if (Only_Unsigned_64_Bit_Ops && Delay_U64_Lowering) {
        SET_OPT_PHASE("U64 lowering in coderep");
        comp_unit->U64_lower_cr(FALSE);  // lowering for unsigned 64-bit ISA
      }

      if (WOPT_Enable_Local_Rvi) {
	SET_OPT_PHASE("Local RVI");
	comp_unit->Do_local_rvi();

        if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
	  fprintf(TFile,"%sAfter Do_local_rvi\n%s", DBar, DBar);
          comp_unit->Htable()->Print(TFile);
	  comp_unit->Cfg()->Print(TFile);
        }

#ifdef Is_True_On  
        SET_OPT_PHASE("Verify CODEMAP");

        if (!Get_Trace(TKIND_INFO, TINFO_TIME)) 
          Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));
#endif // Is_True_On

        if (!Get_Trace(TKIND_INFO, TINFO_TIME)) {
          SET_OPT_PHASE("Verify Live-Range");
          comp_unit->Verify_version();
        }
      }
      else comp_unit->Find_lr_shrink_cand();

      if (WOPT_Enable_Load_PRE) {
        SET_OPT_PHASE("Load PRE");
	comp_unit->Htable()->Verify_var_phi_hash();
	comp_unit->Do_load_pre(WOPT_Enable_Const_PRE /*do_consts*/,
			       !WOPT_Enable_Lpre_Before_Ivr /*do_loads*/);

        if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
          fprintf(TFile, "%sAfter Load PRE\n%s", DBar, DBar);
          comp_unit->Htable()->Print(TFile);
        }

#ifdef Is_True_On  
        SET_OPT_PHASE("Verify CODEMAP");

        if (Get_Trace(TKIND_INFO, TINFO_TIME)) 
          SET_OPT_PHASE("Skip verify CODEMAP because timing trace is on");
        else
          Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));
#endif // Is_True_On
        if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
          SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
        } else {
          SET_OPT_PHASE("Verify Live-Range");
          comp_unit->Verify_version();
        }
      }

      if (WOPT_Enable_Store_PRE && !WOPT_Enable_Spre_Before_Ivr) {
        SET_OPT_PHASE("Store PRE");
	comp_unit->Htable()->Verify_var_phi_hash();
        comp_unit->Do_store_pre();

        if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
          fprintf(TFile, "%sAfter Store PRE\n%s", DBar, DBar);
          comp_unit->Htable()->Print(TFile);
        }

#ifdef Is_True_On  
        SET_OPT_PHASE("Verify CODEMAP");

        if (Get_Trace(TKIND_INFO, TINFO_TIME)) 
          SET_OPT_PHASE("Skip verify CODEMAP because timing trace is on");
        else {
          Is_True(comp_unit->Verify_CODEMAP(), ("CODEMAP corrupted."));
	  comp_unit->Htable()->Verify_hashing();
	}
#endif // Is_True_On
        if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
          SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
        } else {
          SET_OPT_PHASE("Verify Live-Range");
          comp_unit->Verify_version();
        }
      }

      if (WOPT_Enable_Bitwise_DCE) {
        SET_OPT_PHASE("Bitwise DCE");
        comp_unit->Do_bitwise_dce(TRUE /* copy propatage on */);
      }

#if defined(TARG_SL)
      SET_OPT_PHASE("LOCAL_CLSC");
      if (WOPT_Enable_Local_Clsc && (!WOPT_Enable_RVI1)) {
        LOCAL_CLSC lclsc(comp_unit->Cfg(), comp_unit->Opt_stab());
        lclsc.Do_local_clsc();
        if (Get_Trace(TP_GLOBOPT, CR_DUMP_FLAG)) {
	  fprintf(TFile,"%sAfter lclsc\n%s", DBar, DBar);
          comp_unit->Htable()->Print(TFile);
	  comp_unit->Cfg()->Print(TFile);
        }
      }
#endif
    }  // WOPT_Enable_SSA_PRE

    if (OPT_Enable_WHIRL_SSA) {
      // remove dead ssa nodes before emit WHIRL
      comp_unit->Do_dead_code_elim(TRUE, TRUE, TRUE, TRUE,
				   TRUE,
				   FALSE,
				   NULL);
      Is_True(comp_unit->Verify_IR(comp_unit->Cfg(),comp_unit->Htable(),7),
	      ("Verify CFG wrong after creating RVI instance"));
    }

    if (WOPT_Enable_RVI) {
      // Hacky invocation of new PRE RVI hooks for testing. TODO:
      // Clean this up.
      SET_OPT_PHASE("RVI hooks for SSA PRE");
      comp_unit->
	Set_pre_rvi_hooks(CXX_NEW(PRE_RVI_HOOKS(comp_unit->Opt_stab(),
						comp_unit->Cfg(),
						&Opt_local_pool,
						Get_Trace(TP_GLOBOPT,
							  EPRE_DUMP_FLAG)),
				  &Opt_local_pool));
    }

    // create RVI instance before emitting anything
    RVI rvi(WOPT_Enable_RVI, comp_unit->Opt_stab(),
	    WOPT_Enable_RVI ? comp_unit->Pre_rvi_hooks()->Nbits() : 0,
	    alias_mgr );

    Is_True(comp_unit->Verify_IR(comp_unit->Cfg(),comp_unit->Htable(),6),
	    ("Verify CFG wrong after creating RVI instance"));

    if (Get_Trace(TP_GLOBOPT, ENABLE_STAT)) {
      comp_unit->Collect_statistics();
    }

    if (WOPT_Enable_ZDL) {
      SET_OPT_PHASE("ZDL transformation");
      comp_unit->Do_zdl(&rvi);
    }
    
    SET_OPT_PHASE("MainOpt emitter");

    if ( comp_unit->Cfg()->Feedback() )
      comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					    "before MainOpt emitter" );
    if (comp_unit->Cfg()->Feedback() &&
	comp_unit->Cfg()->Feedback()->Trace())
      comp_unit->Cfg()->Feedback()->Print( TFile );

    // if ipsa cg is on, emit whirl later in IPSA::Emit phase
    if(Run_ipsacomp) {
      opt_wn = wn_orig;
      comp_unit->Cfg()->Analyze_loops();
    } else {
      opt_wn = comp_unit->Emit_ML_WHIRL(&rvi);
    }
    if (Cur_PU_Feedback)
      Cur_PU_Feedback->Reset_Root_WN(opt_wn);

    Is_True(REGION_consistency_check(opt_wn),(""));
    Is_True(Verify_alias(alias_mgr,opt_wn),(""));

#if !defined(TARG_IA32) && !defined(TARG_X8664) && !defined(TARG_UWASM)
    if ( WOPT_Enable_RVI ) {
      SET_OPT_PHASE("RVI");
      opt_wn = rvi.Perform_RVI( opt_wn, alias_mgr );
      if (Cur_PU_Feedback)
	Cur_PU_Feedback->Reset_Root_WN(opt_wn);
      Is_True(REGION_consistency_check(opt_wn),(""));
      Is_True(Verify_alias(alias_mgr,opt_wn),(""));
    }
#endif

    // free up optimizer's pools
    // NOTE that the rvi phase uses its own
    Manage_pu_level_memory(comp_unit, &Opt_global_pool);

  } /* if ( phase == MAINOPT_PHASE ) */
  else { 
    if (WOPT_Enable_Loop_Multiver) {
      LOOP_MULTIVER lm (comp_unit);
      lm.Perform_loop_multiversioning ();
    }

    if (phase == PREOPT_LNO_PHASE && WOPT_Enable_Multiver_and_Unroll_Opt) {
        LOOP_PEEL_UNROLL_DRIVER peel_unroller (comp_unit,
            LOOP_PEEL_UNROLL_DRIVER::LPU_OPT_MV_FULLY_UNROLL);
        peel_unroller.Perform_peeling_or_unroll ();
    }

    if (WOPT_Enable_Useless_Store_Elimination)
    {
        
        SET_OPT_PHASE("Useless Store Elimination");

        UselessStoreElimination ulse(comp_unit);
        ulse.Perform_Useless_Store_Elimination();
    }

    SET_OPT_PHASE("Emitter");

    if ( comp_unit->Cfg()->Feedback() )
      comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					    "before emitter" );

    if (This_preopt_renumbers_pregs(phase)) {
      comp_unit->Emitter()->Preg_renumbering_map().Init();
    }

    if (OPT_Enable_WHIRL_SSA) {
#ifdef Is_True_On
      // WSSA, verify CODEREP before emitter
      comp_unit->Htable()->Verify_var_phi_hash();
      comp_unit->Verify_version();
      comp_unit->Verify_CODEMAP();
#endif
    }
    {
      // trace CODEREP before WSSA emitter
      if ( Get_Trace(TP_WSSA, TT_WSSA_EMT) ) {
        fprintf( TFile, "%sCODEREP before WSSA Emitter\n%s", DBar, DBar );
        comp_unit->Htable()->Print(TFile);
        comp_unit->Cfg()->Print(TFile);
      }
    }

    opt_wn = comp_unit->Emitter()->Emit(comp_unit, du_mgr, alias_mgr);
    if (Cur_PU_Feedback)
      Cur_PU_Feedback->Reset_Root_WN(opt_wn);

#ifdef BUILD_MASTIFF
    /* remove dead STPATH after PREOPT emit */
    if (IPSA_insession()) {
      comp_unit->Dna()->Remove_dead_stpath();
    }
#endif

    if (OPT_Enable_WHIRL_SSA) {
#ifdef Is_True_On
      // Verify WHIRL SSA here
#endif
    }
    {
      // trace WHIRL SSA after WSSA emitter
      if ( Get_Trace(TP_WSSA, TT_WSSA_EMT) ) {
        fprintf( TFile, "%sWHIRL after WSSA Emitter\n%s", DBar, DBar );
        fdump_tree( TFile, opt_wn );
      }
    }

    // *****************************************************************
    //    Invoke IPA summary phase
    //      access to comp_unit->Emitter()
    //      and du_mgr.
    // *****************************************************************

    if (Run_ipl) {
      Set_Error_Phase ( "IPA Summary" );
      WB_IPL_Save(); 
      WB_IPL_Initialize(opt_wn, du_mgr, alias_mgr);
      Perform_Procedure_Summary_Phase (opt_wn, du_mgr, alias_mgr,
				       comp_unit->Emitter());
      WB_IPL_Terminate();
      WB_IPL_Restore(); 
    }

    // If we are running preopt for LNO (usually this will be -O3) and
    // we aren't going to run IPA, we re-run alias classification now
    // that we've had the opportunity to renumber PREGs. We don't
    // re-run alias classification here in the IPA case because
    // renumbering will have taken place in IPL's preopt so we
    // (probably?) don't benefit (much?) from redoing it. This second
    // pass of alias classification will (hopefully) let LNO see
    // better (and sometimes far better) alias information, especially
    // for malloc()'ed arrays and other constructs that whose alias
    // behavior was hazy because of the front end's reuse of PREGs.

    if (phase == PREOPT_LNO_PHASE &&
	This_preopt_renumbers_pregs(phase) &&
	WOPT_Enable_Second_Alias_Class &&
	!REGION_has_black_regions(comp_unit->Rid())) {
      SET_OPT_PHASE("Repeat alias classification for LNO");
      alias_mgr->Forget_alias_class_info();
      OPT_POOL_Initialize(&alias_class_pool, "Alias classification pool",
			  FALSE, MEM_DUMP_FLAG+20);
      ALIAS_CLASSIFICATION ac(comp_unit->Opt_stab(),
			      AC_DESTINATION_ALIAS_MANAGER,
			      &alias_class_pool);
      ac.Classify_memops(opt_wn);
      // Now transfer the new alias class information into the alias
      // manager's array of POINTS_TO's, cloning where needed to
      // distinguish things that weren't distinguished before.
      alias_mgr->Transfer_alias_class_to_alias_manager(ac, opt_wn);
      ac.Release_resources();
    }

    if (This_preopt_renumbers_pregs(phase)) {
      Set_PU_Info_flags(Current_PU_Info, PU_PREGS_RENUMBERED);
    }

    // Identify redudant mem clears that follow a calloc and remove them
    remove_redundant_mem_clears(opt_wn, alias_mgr, du_mgr);

    Manage_pu_level_memory(comp_unit, &Opt_global_pool);

    if (WN_opcode(opt_wn) == OPC_FUNC_ENTRY)
      Verify_SYMTAB (CURRENT_SYMTAB);
  }

  /* opt_wn now has result, set the RID level */
  RID *rid = REGION_get_rid(opt_wn);
  Is_True(rid != NULL, ("Pre_Optimizer, NULL RID after processing"));
  RID_level(rid) = RID_preopt_level(phase);

  SET_OPT_PHASE("Finalize");

  REPORT_STATISTICS();

  if (phase == MAINOPT_PHASE &&
      Current_Dep_Graph != NULL && Get_Trace(TP_LNOPT, 1)) {
    /* Trace LNO graph for CG again, since WN addresses have changed */
    fprintf(TFile, "%sLNO dep graph for CG, after WOPT\n%s", DBar, DBar);
    Current_Dep_Graph->Print(TFile);
    fprintf(TFile, "%s", DBar);
  }

  Is_True(REGION_consistency_check(opt_wn),(""));

  if (Get_Trace( TP_GLOBOPT, ALIAS_DUMP_FLAG)){
    Dump_alias_mgr(alias_mgr, opt_wn, TFile);
  }

  disable_tree_freq_display();  // disable WHIRL tree frequency display

  WN_verifier(opt_wn);

  if (WN_opcode(opt_wn) == OPC_FUNC_ENTRY)
    Set_PU_Info_tree_ptr (Current_PU_Info, opt_wn);

  WN_CopyMap(opt_wn, WN_MAP_FEEDBACK, wn_orig);

  return opt_wn;
}

// Proactive loop optimizer.
WN * 
Proactive_Optimizer(OPT_PHASE phase, WN *wn_tree, DU_MANAGER *du_mgr,
		    ALIAS_MANAGER *alias_mgr)
{
  if (Get_Trace(TP_GLOBOPT, -1)) 
    fprintf (TFile,  "\t Proactive Optimizer phase\n");

  Is_True(WN_opcode(wn_tree)==OPC_FUNC_ENTRY || WN_opcode(wn_tree)==OPC_REGION,
	  ("Proactive Optimizer, unknown WHIRL entry point"));
  
  // sets Opt_current_pu_st static
  Opt_set_current_pu_name(wn_tree);

  WN *wopt_pragma = (WN_opcode(wn_tree) == OPC_FUNC_ENTRY) ? 
    WN_func_pragmas(wn_tree) : WN_region_pragmas(wn_tree);
  INT32 pragma_flags = 0;
  BOOL  disable_parm_alias = FALSE;
  for (wopt_pragma = WN_first(wopt_pragma);
       wopt_pragma != NULL;
       wopt_pragma = WN_next(wopt_pragma)) {
	 if ( WN_pragma(wopt_pragma) == WN_PRAGMA_WOPT_FINISHED_OPT ) {
	   pragma_flags = WN_pragma_arg2(wopt_pragma);
	 }
       }

  if (IS_FORTRAN && PU_args_aliased(Pu_Table[Opt_current_pu])) {
    disable_parm_alias = TRUE;
  }

  WOPT_SWITCHES WOPT_Enable(phase, pragma_flags, disable_parm_alias);

  // A nested function that is not MP does not inherit the 
  // restricted mapping. e.g. varfmt nested functions.
  if (WN_opcode(wn_tree) != OPC_REGION) {
    if (PU_is_nested_func(Pu_Table[Opt_current_pu]) &&
	!PU_mp(Get_Current_PU()))
      WOPT_Enable_Restricted_Map = FALSE;
  }

  BOOL Fold_ILOAD_save = WN_Simp_Fold_ILOAD;
  BOOL Fold_LDA_save = WN_Simp_Fold_LDA;

  enable_tree_freq_display();  // enable frequency display for ascii WHIRL dumps
  Init_opt_memory_pools();

  // allocate space for cfg, htable, and itable
  COMP_UNIT *comp_unit = CXX_NEW(COMP_UNIT(wn_tree, alias_mgr,
					   phase, &Opt_global_pool, &Opt_local_pool),
				 &Opt_global_pool);
//#ifdef Is_True_On
  g_comp_unit = comp_unit;
//#endif

  REGION_LEVEL rgn_level = RID_preopt_level(phase);

  // create aux symbol table
  // cannot print WHIRL tree after this point, use dump_tree_no_st
  SET_OPT_PHASE("Create AUX Symbol table");
  WN_Simplifier_Enable(TRUE);	// so that I can fold ILOAD-LDA
  comp_unit->Opt_stab()->Create(comp_unit, rgn_level);
  
  MEM_POOL alias_class_pool;

  OPT_POOL_Initialize(&alias_class_pool, "Alias classification pool",
		      FALSE, MEM_DUMP_FLAG+20);

  ALIAS_CLASSIFICATION ac(comp_unit->Opt_stab(),
			  AC_DESTINATION_OPT_STAB,
			  &alias_class_pool);

  comp_unit->Opt_stab()->Set_alias_classification(ac);

  if (WOPT_Enable_Alias_Classification &&
      !REGION_has_black_regions(comp_unit->Rid())) {
    SET_OPT_PHASE("Compute alias classification");
    ac.Classify_memops(comp_unit->Input_tree());
    comp_unit->Opt_stab()->Incorporate_alias_class_info();
  }
  
  WN_Simplifier_Enable(FALSE);
  WN_Simp_Fold_ILOAD = Fold_ILOAD_save;;
  WN_Simp_Fold_LDA = Fold_LDA_save;;

  // create control flow graph
  SET_OPT_PHASE("Create CFG");

  comp_unit->Cfg()->Create(comp_unit->Input_tree(), FALSE,
			   WOPT_Enable_Calls_Break_BB,
			   rgn_level,
			   comp_unit->Opt_stab(), FALSE,
			   Malloc_Mem_Pool);

  // Transfer feedback data from Whirl annotation (at Cur_PU_Feedback)
  // to optimizer CFG annotation (at comp_unit->Cfg()->Feedback())
  if (Cur_PU_Feedback) {
    SET_OPT_PHASE("Annotate CFG with feedback from Whirl");
    OPT_FEEDBACK *feedback = CXX_NEW(OPT_FEEDBACK(comp_unit->Cfg(),
						  &Opt_global_pool),
				     &Opt_global_pool);
    comp_unit->Cfg()->Set_feedback( feedback );
    comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					  "after CFG Annotation" );
  }
  
  SET_OPT_PHASE("Control Flow Analysis");
  comp_unit->Cfg()->Compute_dom_tree(TRUE); // create dominator tree
  comp_unit->Cfg()->Compute_dom_tree(FALSE); // create post-dominator tree
  comp_unit->Cfg()->Remove_fake_entryexit_arcs();
  comp_unit->Cfg()->Compute_dom_frontier(); // create dominance frontier
  comp_unit->Cfg()->Compute_control_dependence(); // create control-dependence set

  SET_OPT_PHASE("Proactive Loop Transformation");
  comp_unit->Pro_loop_trans();
  comp_unit->Cfg()->Analyze_loops();
  
  // Set up flow free alias information  --  CHI and MU list 
  SET_OPT_PHASE("Create MU and CHI list");
  comp_unit->Opt_stab()->Compute_FFA(comp_unit->Rid());

  // Now the OCC_TAB_ENTRY for each indirect memop is set up, and the
  // correspondence between memops and POINTS_TO's is finalized. We
  // should delay alias classification until now so it can fill in the
  // _alias_class field of each POINTS_TO as it finalizes the
  // equivalence class for each memop.
  //
  // For the sake of not changing too much at once, though, I'm
  // leaving it above for now.

  Is_True(comp_unit->Cfg()->Verify_cfg(),
	  ("Verify CFG wrong after MU and CHI"));

  SET_OPT_PHASE("Create SSA Representation");
  // create ssa representation
  comp_unit->Ssa()->Construct(comp_unit->Htable(),
			      comp_unit->Cfg(),
			      comp_unit->Opt_stab());

  comp_unit->Opt_stab()->Alias_classification()->Release_resources();

  SET_OPT_PHASE("Dead Store Elimination");
  comp_unit->Ssa()->Dead_store_elim(comp_unit);

  comp_unit->Opt_stab()->Update_return_mu();
  
  if (WOPT_Enable_Zero_Version) {
    SET_OPT_PHASE("Find Zero Versions");
    comp_unit->Ssa()->Find_zero_versions();
  }

  SET_OPT_PHASE("Create CODEMAP Representation");
  comp_unit->Ssa()->Create_CODEMAP();

  if (Get_Trace(TKIND_INFO, TINFO_TIME)) {
    SET_OPT_PHASE("Skip verify Live-Range because timing trace is on");
  } else {
    SET_OPT_PHASE("Verify Live-Range");
    comp_unit->Verify_version();
  }

  SET_OPT_PHASE("Verify DO-loop");
  Detect_invalid_doloops(comp_unit);

  SET_OPT_PHASE("Emitter");

  if ( comp_unit->Cfg()->Feedback() )
    comp_unit->Cfg()->Feedback()->Verify( comp_unit->Cfg(),
					  "before emitter" );

  WN * opt_wn = comp_unit->Emitter()->Emit(comp_unit, du_mgr, alias_mgr);
  if (Cur_PU_Feedback)
    Cur_PU_Feedback->Reset_Root_WN(opt_wn);

  Manage_pu_level_memory(comp_unit, &Opt_global_pool);

  if (WN_opcode(opt_wn) == OPC_FUNC_ENTRY)
    Verify_SYMTAB (CURRENT_SYMTAB);

  /* opt_wn now has result, set the RID level */
  RID *rid = REGION_get_rid(opt_wn);
  Is_True(rid != NULL, ("Pre_Optimizer, NULL RID after processing"));
  RID_level(rid) = RID_preopt_level(phase);

  SET_OPT_PHASE("Finalize");

  REPORT_STATISTICS();

  disable_tree_freq_display();  // disable WHIRL tree frequency display
  WN_verifier(opt_wn);

  if (WN_opcode(opt_wn) == OPC_FUNC_ENTRY)
    Set_PU_Info_tree_ptr (Current_PU_Info, opt_wn);
  
  WN_CopyMap(opt_wn, WN_MAP_FEEDBACK, wn_tree);

  return opt_wn;
}

// complete struct relayout optimization

#include "symtab_access.h"

#define MAX_NUM_COMPLETE_STRUCT_RELAYOUT_CANDIDATES 5
#define MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE 32
static TY_IDX complete_struct_relayout_candidate_ty_idx
  [MAX_NUM_COMPLETE_STRUCT_RELAYOUT_CANDIDATES];
static int complete_struct_relayout_candidate_total_num_fields
  [MAX_NUM_COMPLETE_STRUCT_RELAYOUT_CANDIDATES];
static int complete_struct_relayout_candidate_field_access_array
  [MAX_NUM_COMPLETE_STRUCT_RELAYOUT_CANDIDATES]
  [MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE];
static int num_complete_struct_relayout_candidates = 0;
static int continue_with_complete_struct_relayout_analysis = 1;
static int start_counting = 0;

// This function returns the total number of fields in the input structure.
static int
total_num_fields_in_struct(TY_IDX struct_ty_idx)
{
  int i = 0;
  FLD_ITER field_iter = Make_fld_iter(TY_fld(struct_ty_idx));
  do
  {
    i++;
  } while (!FLD_last_field(field_iter++));
  return i;
}

// This function visits all the loops in the input program and counts how many
// different fields of each structure are accessed in the loops.  The answers
// are stored in some static arrays that are global to all the functions being
// compiled for the file.  After the last function is compiled, another function
// will attempt to choose the best structure for complete_struct_relayout
// optimization.
static void
identify_complete_struct_relayout_candidates(WN *wn)
{
  // comment out the following since this optimization benefits both mso as well
  // as non-mso compilations
  // if (!OPT_Scale)
  //   -mso (multi-core scaling optimization is not on)
  //   continue_with_complete_struct_relayout_analysis = 0;
  if (!(PU_src_lang(Get_Current_PU()) & PU_C_LANG))
    IPA_Enable_Struct_Opt = 0; // only do this for C programs
  if (IPA_Enable_Struct_Opt == 0 || wn == NULL ||
      continue_with_complete_struct_relayout_analysis == 0)
    return;
  if (!OPCODE_is_leaf(WN_opcode(wn)))
  {
    if (start_counting == 1)
    {
      if (WN_operator(wn) == OPR_ILOAD || WN_operator(wn) == OPR_ISTORE)
      {
        if (WN_field_id(wn) != 0 &&
            (TY_kind(WN_ty(wn)) == KIND_STRUCT ||
             (TY_kind(WN_ty(wn)) == KIND_POINTER &&
              TY_kind(TY_pointed(WN_ty(wn))) == KIND_STRUCT)))
        {
          // found an access to "structure.field"; record it
          TY_IDX struct_ty_idx;
          int i;
          int j;
          int n;

          if (TY_kind(WN_ty(wn)) == KIND_STRUCT)
            struct_ty_idx = WN_ty(wn);
          else
            struct_ty_idx = TY_pointed(WN_ty(wn));
          for (i = 0; i < num_complete_struct_relayout_candidates; i++)
          {
            if (complete_struct_relayout_candidate_ty_idx[i] == struct_ty_idx)
            {
              // we have encountered this structure in some loops before; just
              // update the field access
              if (WN_field_id(wn) >=
                  MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE)
              {
                continue_with_complete_struct_relayout_analysis = 0;
                return;
              }
              complete_struct_relayout_candidate_field_access_array[i]
                [WN_field_id(wn)] = 1; // careful, some C++ #fields >= MAX
              break;
            }
          }
          if (i == num_complete_struct_relayout_candidates)
          {
            // we have not encountered this structure in any loops before; enter
            // it into the array
            if (i >= MAX_NUM_COMPLETE_STRUCT_RELAYOUT_CANDIDATES)
            {
              continue_with_complete_struct_relayout_analysis = 0;
              return; // too many such structures encountered in loops; give up
            }
            n = total_num_fields_in_struct(struct_ty_idx);
            if (n >= MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE)
            {
              continue_with_complete_struct_relayout_analysis = 0;
              return; // too many fields in this struct; give up
            }
            complete_struct_relayout_candidate_ty_idx[i] = struct_ty_idx;
            complete_struct_relayout_candidate_total_num_fields[i] = n;
            for (j = 0; j <
                 MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE; j++)
              complete_struct_relayout_candidate_field_access_array[i][j] = 0;
            if (WN_field_id(wn) >=
                MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE)
            {
              continue_with_complete_struct_relayout_analysis = 0;
              return;
            }
            complete_struct_relayout_candidate_field_access_array[i]
              [WN_field_id(wn)] = 1; // careful, some C++ #fields >= MAX
            num_complete_struct_relayout_candidates++;
          }
        }
      }
    }
    if (WN_operator(wn) == OPR_BLOCK)
    {
      WN *child = WN_first(wn);
      while (child != NULL)
      {
        identify_complete_struct_relayout_candidates(child);
        child = WN_next(child);
      }
    }
    else
    {
      INT child_num;
      WN *child;
      if (WN_operator(wn) == OPR_DO_LOOP ||
          WN_operator(wn) == OPR_WHILE_DO ||
          WN_operator(wn) == OPR_DO_WHILE)
        start_counting = 1; // only count structure field accesses in loops
      for (child_num = 0; child_num < WN_kid_count(wn); child_num++)
      {
        child = WN_kid(wn, child_num);
        if (child != NULL)
          identify_complete_struct_relayout_candidates(child);
      }
      if (WN_operator(wn) == OPR_DO_LOOP ||
          WN_operator(wn) == OPR_WHILE_DO ||
          WN_operator(wn) == OPR_DO_WHILE)
        start_counting = 0;
    }
  }
  // not interested in any leaf nodes
  return;
}

// This function is called after the last function in the file has been
// compiled.  Among all the structures that have been marked (more precisely,
// all the structures whose fields that were accessed in some loops have been
// marked), choose one that is the most profitable for complete_struct_relayout
// optimization.
void
choose_from_complete_struct_for_relayout_candidates()
{
  int i;
  int j;
  int num_fields_accessed;

  // in case you think the following is a good structure splitting candidate,
  // and want to perform structure splitting or peeling optimization instead,
  // keep in mind that structure splitting/peeling analyses have already been
  // performed and deemed it not a candidate; we will perform
  // complete_struct_relayout optimization on it instead
  if (IPA_Enable_Struct_Opt == 0 ||
      continue_with_complete_struct_relayout_analysis == 0)
    return; // we have given up already
  for (i = 0; i < num_complete_struct_relayout_candidates; i++)
  {
    num_fields_accessed = 0;
    for (j = 0; j < MAX_NUM_FIELDS_IN_COMPLETE_STRUCT_RELAYOUT_CANDIDATE; j++)
      num_fields_accessed +=
        complete_struct_relayout_candidate_field_access_array[i][j];
    if (num_fields_accessed == 1)
    {
      // wow, only *1* field in this entire structure was accessed in all the
      // loops in all the functions in this file; choose it if the entire
      // structure matches some good heuristics in size and number of fields
      if (complete_struct_relayout_candidate_total_num_fields[i] > 8 &&
          TY_size(complete_struct_relayout_candidate_ty_idx[i]) < 128)
      {
        if (Get_Trace(TP_IPL, 1))
          fprintf(TFile, "ipl -> complete_struct_relayout_candidate = %d %s\n",
            complete_struct_relayout_candidate_ty_idx[i],
            TY_name(complete_struct_relayout_candidate_ty_idx[i]));
        // for some reason when we do the following *another* such type is
        // created, making the complete_struct_relayout_type_id not unique,
        // introducing a lot of confusion
        Set_TY_complete_struct_relayout_candidate
          (complete_struct_relayout_candidate_ty_idx[i]);
      }
    }
  }
  return;
}

// array remapping optimization

#define MAX_NUM_ARRAY_REMAPPING_CANDIDATES 5 // max num of different arrays
#define MAX_NUM_SUBSCRIPTS_IN_ARRAY_REMAPPING_CANDIDATE 128 // max num of static
  // array references in loop (*not* the total num of subscripts in array)
static ST *array_remapping_candidate_st[MAX_NUM_ARRAY_REMAPPING_CANDIDATES];
static int num_subscripts_in_array_remapping_candidate
  [MAX_NUM_ARRAY_REMAPPING_CANDIDATES];
static int array_remapping_candidate_subscript
  [MAX_NUM_ARRAY_REMAPPING_CANDIDATES]
  [MAX_NUM_SUBSCRIPTS_IN_ARRAY_REMAPPING_CANDIDATE];
static int remainder_array[MAX_NUM_SUBSCRIPTS_IN_ARRAY_REMAPPING_CANDIDATE];
static int num_array_remapping_candidates = 0;
static int array_remapping_in_loop = 0;
static int array_remapping_discovered_loop_stride = 0;
static int continue_with_array_remapping_analysis = 1;

// This function visits all the loops in the input program and analyzes the
// array references inside these loops.  A heuristic is used to determine if the
// array in question will likely benefit from the array remapping optimization.
// If so, the array's ST is marked as such.
static void
identify_array_remapping_candidates(WN *wn, ST *loop_index_st)
{
  int i;
  int j;

  if (!OPT_Scale)
    // -mso (multi-core scaling optimization is not on)
    continue_with_array_remapping_analysis = 0;
  if (IPA_Enable_Struct_Opt == 0 || wn == NULL ||
      continue_with_array_remapping_analysis == 0)
    return;
  if (!OPCODE_is_leaf(WN_opcode(wn)))
  {
    if (array_remapping_in_loop == 1)
    {
      // we are in a loop; analyze all iloads and istores (for array[]'s)
      if (WN_operator(wn) == OPR_ILOAD || WN_operator(wn) == OPR_ISTORE)
      {
        ST *array_st;
        int constant1;
        int constant2;
        int element_size;
        int offset;
        WN *wn1;
        int subscript;

        // *(array + (i + constant1) * element_size + constant2 + offset)
        offset = WN_offset(wn);
        if (WN_operator(wn) == OPR_ILOAD)
          wn1 = WN_kid0(wn);
        else
          wn1 = WN_kid1(wn);
        if (WN_operator(wn1) == OPR_ADD &&
            WN_operator(WN_kid0(wn1)) == OPR_ADD &&
            WN_operator(WN_kid1(wn1)) == OPR_INTCONST)
        {
          constant2 = WN_const_val(WN_kid1(wn1));
          wn1 = WN_kid0(wn1);
        }
        else
          constant2 = 0;
        if (WN_operator(wn1) == OPR_ADD &&
            WN_operator(WN_kid0(wn1)) == OPR_LDID &&
            WN_operator(WN_kid1(wn1)) == OPR_MPY &&
            WN_operator(WN_kid1(WN_kid1(wn1))) == OPR_INTCONST)
        {
          array_st = WN_st(WN_kid0(wn1));
          element_size = WN_const_val(WN_kid1(WN_kid1(wn1)));
          wn1 = WN_kid0(WN_kid1(wn1));
          if (WN_operator(wn1) == OPR_CVT)
            wn1 = WN_kid0(wn1);
          if (WN_operator(wn1) == OPR_ADD &&
              WN_operator(WN_kid1(wn1)) == OPR_INTCONST)
          {
            constant1 = WN_const_val(WN_kid1(wn1));
            wn1 = WN_kid0(wn1);
          }
          else
            constant1 = 0;
          if (WN_operator(wn1) == OPR_LDID && WN_st(wn1) == loop_index_st &&
              offset % element_size == 0 && constant2 % element_size == 0)
          {
            // *(array + (i + constant1) * element_size + constant2 + offset)
            // is equivalent to array[(i + constant1) + (constant2 + offset) /
            // element_size], giving the constant subscript portion to be
            // "i + constant1 + (constant2 + offset) / element_size"
            subscript = constant1 + (constant2 + offset) / element_size;

            // record this information
            for (i = 0; i < num_array_remapping_candidates; i++)
            {
              if (array_remapping_candidate_st[i] == array_st)
              {
                // enter it, *even* if it is a duplicate, because the number of
                // occurrences factors in the heuristic of determining if this
                // array is a remapping candidate (see heuristic below)
                array_remapping_candidate_subscript[i]
                  [num_subscripts_in_array_remapping_candidate[i]] = subscript;
                num_subscripts_in_array_remapping_candidate[i]++;
                if (num_subscripts_in_array_remapping_candidate[i] >=
                    MAX_NUM_SUBSCRIPTS_IN_ARRAY_REMAPPING_CANDIDATE)
                {
                  continue_with_array_remapping_analysis = 0;
                  return;
                }
                break;
              }
            }
            if (i == num_array_remapping_candidates)
            {
              // new array remapping candidate
              array_remapping_candidate_st[i] = array_st;
              num_subscripts_in_array_remapping_candidate[i] = 1;
              array_remapping_candidate_subscript[i][0] = subscript;
              num_array_remapping_candidates++;
              if (num_array_remapping_candidates >=
                  MAX_NUM_ARRAY_REMAPPING_CANDIDATES)
              {
                continue_with_array_remapping_analysis = 0;
                return;
              }
            }
          }
        }
      }
      else if (WN_operator(wn) == OPR_STID && WN_st(wn) == loop_index_st &&
               WN_operator(WN_kid0(wn)) == OPR_ADD &&
               WN_operator(WN_kid0(WN_kid0(wn))) == OPR_LDID &&
               WN_st(WN_kid0(WN_kid0(wn))) == loop_index_st &&
               WN_operator(WN_kid1(WN_kid0(wn))) == OPR_INTCONST)
      {
        // approximate loop stride (inside if stmt, multiple increments, etc.
        array_remapping_discovered_loop_stride +=
          WN_const_val(WN_kid1(WN_kid0(wn)));
      }
    }
    if (WN_operator(wn) == OPR_BLOCK)
    {
      WN *child_wn = WN_first(wn);
      while (child_wn != NULL)
      {
        identify_array_remapping_candidates(child_wn, loop_index_st);
        child_wn = WN_next(child_wn);
      }
    }
    else
    {
      ST *local_loop_index_st = loop_index_st;
      if (WN_operator(wn) == OPR_DO_LOOP ||
          WN_operator(wn) == OPR_WHILE_DO ||
          WN_operator(wn) == OPR_DO_WHILE)
      {
        // found a loop, but if it is not the outermost loop, skip it.  For now,
        // we only analyze array subscripts in the outermost loop.  Later, we
        // will add code to process multi-dimensional array subscripts
        if (array_remapping_in_loop == 1)
          return;
        // otherwise, get ready to process this (outermost) loop
        array_remapping_in_loop = 1;
        num_array_remapping_candidates = 0;
        array_remapping_discovered_loop_stride = 0;
        if (WN_operator(wn) == OPR_DO_LOOP)
          local_loop_index_st = WN_st(WN_index(wn));
        else
        {
          // OPR_WHILE_DO, OPR_DO_WHILE
          local_loop_index_st = NULL;
          if (WN_operator(WN_while_test(wn)) == OPR_LE ||
              WN_operator(WN_while_test(wn)) == OPR_LT ||
              WN_operator(WN_while_test(wn)) == OPR_GE ||
              WN_operator(WN_while_test(wn)) == OPR_GT ||
              WN_operator(WN_while_test(wn)) == OPR_EQ ||
              WN_operator(WN_while_test(wn)) == OPR_NE)
          {
            if (WN_operator(WN_kid0(WN_while_test(wn))) == OPR_LDID &&
                WN_operator(WN_kid1(WN_while_test(wn))) == OPR_INTCONST)
              local_loop_index_st = WN_st(WN_kid0(WN_while_test(wn)));
            if (WN_operator(WN_kid1(WN_while_test(wn))) == OPR_LDID &&
                WN_operator(WN_kid0(WN_while_test(wn))) == OPR_INTCONST)
              local_loop_index_st = WN_st(WN_kid1(WN_while_test(wn)));
          }
        }
      }
      for (INT child_num = 0; child_num < WN_kid_count(wn); child_num++)
      {
        WN *child_wn = WN_kid(wn, child_num);
        if (child_wn != NULL)
          identify_array_remapping_candidates(child_wn, local_loop_index_st);
      }
      if (WN_operator(wn) == OPR_DO_LOOP ||
          WN_operator(wn) == OPR_WHILE_DO ||
          WN_operator(wn) == OPR_DO_WHILE)
      {
        int max_subscript;
        int min_subscript;
        int remainder;

        // we have just finished processing a loop; are there any candidates for
        // array remapping?
        if (array_remapping_discovered_loop_stride <= 0)
          return; // not if we don't understand the loop
        if (array_remapping_discovered_loop_stride >
            MAX_NUM_SUBSCRIPTS_IN_ARRAY_REMAPPING_CANDIDATE)
          return; // or that the loop stride (num of distinct remainders) won't
                  // fit into the remainder_array
        for (i = 0; i < num_array_remapping_candidates; i++)
        {
          // this is the heuristic for an array to be selected as a remapping
          // candidate:  the access patterns have to look like:
          // a[subscript_group0], a[subscript_group1], ..., a[subscript_groupn]
          // where n = number of groups = loop stride, and that each group has
          // to occur equally often, and that these subscripts are very far
          // apart, and that the array references in the next loop iteration
          // will not fit in the same cache line as the current iteration.  The
          // rationale is that we want to detect constructs such as:
          //   for (i = 0; i < N; i += loop_stride)
          //   {
          //     ... a[iteration(group0)] ...
          //     ... a[iteration(group1)] ...
          //     ...
          //     ... a[iteration(group(loop_stride-1))] ...
          //   }
          // and remap the array a with all the group0 subscripts together,
          // followed by all the group1 subscripts together, etc.  This way each
          // successive iteration reference to a[iteration(groupx)] will most
          // likely be in the same cache line as the current iteration
          for (j = 0; j < MAX_NUM_SUBSCRIPTS_IN_ARRAY_REMAPPING_CANDIDATE; j++)
            remainder_array[j] = 0; // initialization
          max_subscript = -999999;
          min_subscript = 999999;
          for (j = 0; j < num_subscripts_in_array_remapping_candidate[i]; j++)
          {
            remainder = array_remapping_candidate_subscript[i][j] %
              array_remapping_discovered_loop_stride;
            if (remainder < 0)
              remainder += array_remapping_discovered_loop_stride;
            remainder_array[remainder]++; // we made sure in the above that this
                                          // will fit
            if (array_remapping_candidate_subscript[i][j] > max_subscript)
              max_subscript = array_remapping_candidate_subscript[i][j];
            if (array_remapping_candidate_subscript[i][j] < min_subscript)
              min_subscript = array_remapping_candidate_subscript[i][j];
          }
          if ((max_subscript - min_subscript) < 100000)
            continue; // array_remapping_candidate_st[i] is not a candidate:
                      // the subscripts are not far apart enough
          // by the way, we realize that the initial arbitrary values for
          // max_subscript and min_subscript could have been refined to be the
          // first value of array_remapping_candidate_subscript[i][0] (if the j
          // loop executes), and that in certain cases these max and min values
          // may be incorrectly left unchanged.  This is acceptable, since this
          // is just a heuristic

          // now we make sure that these subscript group numbers occur equally
          // frequently
          remainder = 0;
          for (j = 0; j < array_remapping_discovered_loop_stride; j++)
          {
            if (remainder_array[j] == 0)
              continue;
            if (remainder == 0)
            {
              remainder = remainder_array[j]; // number of occurrences for group
                                              // (remainder) j
              continue;
            }
            if (remainder_array[j] != remainder)
              break; // not all the groups (remainders) occur equally often
          }
          if (j != array_remapping_discovered_loop_stride)
            continue; // array_remapping_candidate_st[i] is not a candidate
          else
          {
            // found a candidate; mark it
            Set_ST_is_array_remapping_candidate(array_remapping_candidate_st
              [i]);
            // indicate that this function has an array remapping candidate
            // (this will speed up the search for such candidates in the ipo
            // phase)
            Set_ST_is_array_remapping_candidate(Get_Current_PU_ST());
            if (Get_Trace(TP_IPL, 1))
              fprintf(TFile, "ipl -> array_remapping_candidate = %s in function %s\n",
                ST_name(array_remapping_candidate_st[i]),
                ST_name(Get_Current_PU_ST()));
          }
        }
        array_remapping_in_loop = 0; // finished processing this loop
      }
    }
  }
  // not interested in any leaf nodes
  return;
}
